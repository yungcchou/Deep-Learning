# 2D Convolutional Neural Network (2D 卷積神經網路)

**卷積運算** (Convolution)，特別是**卷積神經網路**（CNN力Convolutional Neural Network），在各種**深度學習**任務中被廣泛應用。
它們在處理**空間數據**（如影像、視訊，甚至某些類型的時間序列數據）方面特別有效。
在與**全連接神經網路**的比較中，卷積的優勢可以分為幾個重要方面:

1. **局部感受域 (Local Receptive Fields)（獲取空間層次結構）**
   - 卷積透過關注數據的局部區域來捕捉**空間層次結構**和**局部模式**。
   - 對於影像，這意味著模型可以在早期層識別**邊緣**、**紋理**或**簡單形狀**，在更深層次識別更複雜的結構（如物體）。
2. **參數效率（權重共享）**
   - 在全連接網路中：每個神經元與每個輸入特徵相連接，這會導致大量的參數，特別是對於像影像這樣的大輸入（例如，$28\times28$ 的影像有 784 個像素）。
   - **在 CNN 中**：一個單一的過濾器在輸入上滑動（空間上或時間上），並在每個位置應用相同的權重。這顯著減少了參數的數量。
3. **平移不變性**
   - 卷積網路可以無論特徵出現在輸入的何處都能檢測到它們。如果一個特徵（如邊緣或模式）出現在影像的不同位置，卷積過濾器可以在任何地方識別它。
4. **層次化特徵學習**
   - 在 CNN 中，靠近輸入的層學習檢測低級特徵，如邊緣、角點和紋理，而較深層次則捕捉更高級的特徵，如形狀、模式和物體。
   - 這種層次結構允許 CNN 建立越來越抽象的輸入表示。
   - **在視覺任務中**：第一層可能檢測邊緣或小紋理，而後面的層則檢測人臉、動物或整個物體。
5. **處理高維數據**
   - 卷積層通過池化和卷積運算逐步減少輸入數據的維度，從而在不丟失重要空間信息的情況下，更容易處理高維數據，如大影像或視訊。
   - 池化層：這些層透過下採樣（例如最大池化）來降低特徵圖的解析度，從而減少計算成本，同時保留最重要的特徵。
6. **更少的參數需要學習**
   - 對於大輸入，全連接層因參數激增而迅速變得難以處理，但 CNN 透過權重共享學習的參數更少。這減少了過擬合的風險並加快了訓練速度。
   - 例如：對於大小為 $28 \times 28 \times 3$（RGB 通道）的影像，全連接層僅一層就需要 $784 \times 3 \times N$ 個參數，其中 $N$ 是神經元的數量。而卷積層使用大小為 $3 \times 3$ 的濾波器，則每個濾波器只需要 $3 \times 3 \times 3 = 27$ 個參數。
7. **結構化數據：網格狀拓撲結構**  
   - 卷積運算特別適合處理網格狀的**拓撲結構**，例如影像（2D 網格）或視訊（3D 網格，時間為第三維度）。  
   - 它們能夠在局部區域內**滑動**並捕捉模式，使其成為處理這類數據的理想選擇。  
   - **應用領域**：卷積通常應用於影像處理（如**物體檢測**、**分割**）、**視訊分析**以及某些 1D 任務，如時間序列預測（在這種情況下可以使用 1D 卷積）。

8. **正則化：減少過擬合**  
   - 通過使用**小濾波器**和**局部連接**，CNN 對學習過程施加了強烈的**先驗約束**。這可以**減少過擬合**的風險，尤其是在與可能通過學習遙遠輸入間的偶然相關性而過擬合的全連接網絡相比時。

9. **比例與變形不變性（使用先進架構）**  
   - 雖然基本的 CNN 可以捕捉**平移不變性**，但一些先進技術，如**空間變換網絡**和**膨脹卷積**，讓 CNN 在面對輸入數據的**比例**、**旋轉**或**變形**變化時更具強韌性。  
   - **應用領域**：在物體檢測任務中，當物體以不同比例或方向出現在影像中時，這些技術幫助 CNN 更好地適應。

---

## The core procedures of CNNs

CNN的核心過程涉及透過卷積層、池化層和全連接層提取層次化的特徵。以下是CNN過程的具體說明：

### 1. **輸入數據**

對於CNN，輸入數據通常是多維的：

- **圖像**：一個2D或3D的像素值陣列。
  - 對於灰度圖像：輸入大小為 $H \times W \times 1$ （高度，寬度，1個通道）。
  - 對於RGB圖像：輸入大小為 $H \times W \times 3$ （高度，寬度，3個通道）。

給定一個高度為 $H$、寬度為 $W$ 並有 $C$ 個通道的圖像，輸入可以表示為：
$$X = \{x_{h,w,c}\}, \quad h \in [0, H-1], w \in [0, W-1], c \in [0, C-1]$$

其中， $x_{h,w,c}$ 是在位置 $(h, w)$ 通道 $c$ 上的像素值。

### 2. **卷積層**

卷積層使用濾波器（或稱為內核）來提取局部特徵。濾波器是一個小型的權重矩陣，在輸入上滑動並在每個位置計算點積。

- **卷積操作**：對於2D卷積，給定輸入 $X$ 和濾波器 $F$，卷積操作會生成一個特徵圖 $Y$。假設輸入的尺寸為 $H \times W \times C$，濾波器的尺寸為 $k_H \times k_W \times C$（濾波器的高度、寬度和深度）。

在每個位置 $(h, w)$，卷積輸出的計算方式為：

$$Y_{h, w} = \sum_{i=0}^{k_H-1} \sum_{j=0}^{k_W-1} \sum_{c=0}^{C-1} F_{i,j,c} \cdot X_{h+i, w+j, c}$$

其中 $F$ 是濾波器內核，輸出 $Y$ 是生成的特徵圖。該操作在整個輸入上重複進行，根據指定的步幅滑動濾波器。

- **步幅**：步幅控制濾波器在輸入上移動的方式。步幅為1表示濾波器每次移動1個像素，而較大的步幅表示濾波器每次移動更多個像素。
  
- **填充**：填充用於控制輸出的大小。填充 $p$ 會在輸入周圍添加零邊框：
  - **Valid填充**（無填充）：輸出尺寸縮小。
  - **Same填充**（零填充）：輸出尺寸與輸入相同。

### 3. **激活函數 (ReLU)**

在應用卷積操作後，會應用激活函數（通常是ReLU：修正線性單元）來引入非線性。ReLU定義為：

$$f(x) = \max(0, x)$$

這個操作將特徵圖中的所有負值替換為零，幫助網絡建模非線性關係。

### 4. **池化層**

池化層減少特徵圖的空間維度（高度和寬度），同時保留最重要的信息。這有助於減少計算複雜度並防止過擬合。

- **最大池化**：最常見的池化操作是最大池化，在每個由池化濾波器覆蓋的區域中取最大值。如果我們使用 $2 \times 2$ 的池化窗口，且步幅為2，則輸出為：

$$Y_{h, w} = \max(X_{h+i, w+j}), \quad i, j \in [0, 1]$$

其中， $Y$ 是下採樣後的特徵圖， $X$ 是輸入特徵圖。

- **平均池化**：取濾波器覆蓋區域的平均值，而非最大值。

### 5. **全連接層（密集層）**

經過一系列的卷積和池化層後，生成的特徵圖會被展平為一個1D向量，然後傳遞到全連接層，進行最終的分類。

令展平後的輸入向量為 $z$。全連接層的計算方式為：

$$y = Wz + b$$

其中：

- $W$ 是權重矩陣。
- $b$ 是偏置向量。
- $y$ 是輸出（對數機率）。

### 6. **輸出層（Softmax）**

CNN的最後一層通常是用於多分類的Softmax層。Softmax函數將對數機率轉換為概率，確保所有類別的概率總和為1：

$$P(y=k) = \frac{e^{z_k}}{\sum_{i=1}^{K} e^{z_i}}$$

其中 $z_k$ 是類別 $k$ 的對數機率，$K$ 是總類別數。

### 7. **損失函數**

對於分類任務，常用的損失函數是分類交叉熵：
$$L(y, \hat{y}) = -\sum_{i=1}^{K} y_i \log(\hat{y}_i)$$
其中：

- $y_i$ 是真實標籤（獨熱編碼）。
- $\hat{y}_i$ 是類別 $i$ 的預測概率。

### 8. **反向傳播與優化**

一旦完成CNN的前向傳播並計算出損失，模型的權重會使用 **反向傳播** 和優化演算法（如 **隨機梯度下降** 或 **Adam**）來更新。

- **反向傳播** 利用鏈式法則計算損失函數對每個權重的梯度。
- **優化器** 通過更新權重來最小化損失。

### CNN操作的關鍵步驟總結

1. **卷積**：透過濾波器提取局部特徵。
2. **激活**：引入非線性（通常是ReLU）。
3. **池化**：降低空間維度，同時保留最重要的信息。
4. **展平**：將2D特徵圖轉換為1D向量。
5. **全連接層**：綜合所有特徵並進行分類。
6. **Softmax輸出**：將對數機率轉換為類別概率。
7. **反向傳播與優化**：最小化損失並更新權重。

### 一個簡單的CNN圖像分類架構範例

1. 輸入： $28 \times 28 \times 1$（灰度圖像）。
2. 卷積層 1：32個 $3 \times 3$ 濾波器。
3. ReLU激活。
4. 最大池化： $2 \times 2$ 窗口。
5. 卷積層 2：64個 $3 \times 3$ 濾波器。
6. ReLU激活。
7. 最大池化： $2 \times 2$ 窗口。
8. 展平層：將2D特徵圖展平為1D。
9. 全連接層（密集層）：128個單元。
10. 輸出層（Softmax）：10個單元，用於10個類別的分類。

這個過程使CNN能夠高效地學習空間層次結構和模式，特別適合於圖像、視頻以及結構化數據的處理。
