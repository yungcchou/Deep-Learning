{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用循環神經網絡（RNN）自然語言處理（NLP）\n",
    "\n",
    "### Step 1：**資料準備**\n",
    "1. **資料清理**：\n",
    "   - 移除雜訊，如特殊字符、URL、不必要的空格等。\n",
    "2. **分詞（Tokenization）**：\n",
    "   - 將文本分割成詞或子詞（如 WordPiece、BPE）。\n",
    "3. **編碼（Encoding）**：\n",
    "   - 將分詞結果轉換為數字表示。\n",
    "   - 常見的方法包括：\n",
    "     - One-hot 編碼\n",
    "     - 詞嵌入（如 Word2Vec、GloVe）\n",
    "     - 預訓練嵌入（如 BERT 或 GPT）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "    Check out this amazing blog at https://example.com!\n",
      "    Contact us via email: test@example.com or call +123-456-7890.\n",
      "    #Python #NLP :)\n",
      "    \n",
      "\n",
      "Cleaned Text:\n",
      " check out this amazing blog at contact us via email testexamplecom or call python nlp\n"
     ]
    }
   ],
   "source": [
    "# 資料清理\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    Check out this amazing blog at https://example.com!\n",
    "    Contact us via email: test@example.com or call +123-456-7890.\n",
    "    #Python #NLP :)\n",
    "    \"\"\"\n",
    "    cleaned_text = clean_text(sample_text)\n",
    "    print(\"Original Text:\\n\", sample_text)\n",
    "    print(\"\\nCleaned Text:\\n\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分詞（Tokenization）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "    Check out this amazing blog at https://example.com!\n",
      "    Contact us via email: test@example.com or call +123-456-7890.\n",
      "    #Python #NLP :)\n",
      "    \n",
      "\n",
      "Cleaned Text:\n",
      " check out this amazing blog at contact us via email testexamplecom or call python nlp\n",
      "\n",
      "Tokenized Text:\n",
      " ['check', 'out', 'this', 'amazing', 'blog', 'at', 'contact', 'us', 'via', 'email', 'testexamplecom', 'or', 'call', 'python', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing URLs, special characters, and unnecessary whitespaces.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the cleaned text into words using Python's built-in methods.\n",
    "    \"\"\"\n",
    "    # Split text by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    Check out this amazing blog at https://example.com!\n",
    "    Contact us via email: test@example.com or call +123-456-7890.\n",
    "    #Python #NLP :)\n",
    "    \"\"\"\n",
    "    print(\"Original Text:\\n\", sample_text)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(sample_text)\n",
    "    print(\"\\nCleaned Text:\\n\", cleaned_text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenize_text(cleaned_text)\n",
    "    print(\"\\nTokenized Text:\\n\", tokenized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 編碼（Encoding）-- One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "    Check out this amazing blog at https://example.com!\n",
      "    Contact us via email: test@example.com or call +123-456-7890.\n",
      "    #Python #NLP :)\n",
      "    \n",
      "\n",
      "Cleaned Text:\n",
      " check out this amazing blog at contact us via email testexamplecom or call python nlp\n",
      "\n",
      "Tokenized Text:\n",
      " ['check', 'out', 'this', 'amazing', 'blog', 'at', 'contact', 'us', 'via', 'email', 'testexamplecom', 'or', 'call', 'python', 'nlp']\n",
      "\n",
      "Word to Index Mapping:\n",
      " {'amazing': 0, 'at': 1, 'blog': 2, 'call': 3, 'check': 4, 'contact': 5, 'email': 6, 'nlp': 7, 'or': 8, 'out': 9, 'python': 10, 'testexamplecom': 11, 'this': 12, 'us': 13, 'via': 14}\n",
      "\n",
      "One-Hot Encoded Vectors:\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing URLs, special characters, and unnecessary whitespaces.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the cleaned text into words using Python's built-in methods.\n",
    "    \"\"\"\n",
    "    # Split text by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def one_hot_encode(tokens):\n",
    "    \"\"\"\n",
    "    Converts a list of tokens into a one-hot encoded representation.\n",
    "    \n",
    "    Parameters:\n",
    "        tokens (list): List of tokens from tokenized text.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A dictionary mapping words to indices and a one-hot encoded list of lists.\n",
    "    \"\"\"\n",
    "    # Create a vocabulary of unique tokens\n",
    "    vocabulary = sorted(set(tokens))\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    \n",
    "    # Generate one-hot encoding for each token\n",
    "    one_hot_vectors = []\n",
    "    for token in tokens:\n",
    "        one_hot_vector = [0] * len(vocabulary)\n",
    "        one_hot_vector[word_to_index[token]] = 1\n",
    "        one_hot_vectors.append(one_hot_vector)\n",
    "    \n",
    "    return word_to_index, one_hot_vectors\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    Check out this amazing blog at https://example.com!\n",
    "    Contact us via email: test@example.com or call +123-456-7890.\n",
    "    #Python #NLP :)\n",
    "    \"\"\"\n",
    "    print(\"Original Text:\\n\", sample_text)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(sample_text)\n",
    "    print(\"\\nCleaned Text:\\n\", cleaned_text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenize_text(cleaned_text)\n",
    "    print(\"\\nTokenized Text:\\n\", tokenized_text)\n",
    "\n",
    "    # One-hot encode the tokens\n",
    "    word_to_index, one_hot_vectors = one_hot_encode(tokenized_text)\n",
    "    print(\"\\nWord to Index Mapping:\\n\", word_to_index)\n",
    "    print(\"\\nOne-Hot Encoded Vectors:\")\n",
    "    for vector in one_hot_vectors:\n",
    "        print(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 編碼 (Encoding) -- 詞嵌入（如 Word2Vec、GloVe）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "    Check out this amazing blog at https://example.com!\n",
      "    Contact us via email: test@example.com or call +123-456-7890.\n",
      "    #Python #NLP :)\n",
      "    \n",
      "\n",
      "Cleaned Text:\n",
      " check out this amazing blog at contact us via email testexamplecom or call python nlp\n",
      "\n",
      "Tokenized Text:\n",
      " ['check', 'out', 'this', 'amazing', 'blog', 'at', 'contact', 'us', 'via', 'email', 'testexamplecom', 'or', 'call', 'python', 'nlp']\n",
      "\n",
      "Word2Vec Embeddings:\n",
      "nlp: [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
      "python: [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
      "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
      " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
      "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
      "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
      " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
      " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
      " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
      " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
      "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
      " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
      "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
      "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
      " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
      "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
      "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
      "call: [ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      "or: [-8.2440982e-03  9.2991991e-03 -1.9725331e-04 -1.9685079e-03\n",
      "  4.6036434e-03 -4.0951595e-03  2.7420460e-03  6.9410047e-03\n",
      "  6.0661924e-03 -7.5107221e-03  9.3817133e-03  4.6716975e-03\n",
      "  3.9660614e-03 -6.2439428e-03  8.4604742e-03 -2.1511274e-03\n",
      "  8.8242898e-03 -5.3622089e-03 -8.1289113e-03  6.8254727e-03\n",
      "  1.6703474e-03 -2.1991611e-03  9.5135355e-03  9.4952369e-03\n",
      " -9.7737284e-03  2.5051939e-03  6.1564865e-03  3.8730367e-03\n",
      "  2.0229670e-03  4.3106405e-04  6.7346974e-04 -3.8211411e-03\n",
      " -7.1399156e-03 -2.0880306e-03  3.9246641e-03  8.8188052e-03\n",
      "  9.2591848e-03 -5.9765638e-03 -9.4021866e-03  9.7645782e-03\n",
      "  3.4283167e-03  5.1672393e-03  6.2813754e-03 -2.8041822e-03\n",
      "  7.3210048e-03  2.8288446e-03  2.8727155e-03 -2.3800777e-03\n",
      " -3.1294629e-03 -2.3699196e-03  4.2772293e-03  7.5820331e-05\n",
      " -9.5850844e-03 -9.6653933e-03 -6.1471672e-03 -1.2862809e-04\n",
      "  1.9972187e-03  9.4322599e-03  5.5857045e-03 -4.2903535e-03\n",
      "  2.7716716e-04  4.9646846e-03  7.6989033e-03 -1.1450007e-03\n",
      "  4.3227156e-03 -5.8153337e-03 -8.0281531e-04  8.0997068e-03\n",
      " -2.3602233e-03 -9.6629262e-03  5.7801716e-03 -3.9290586e-03\n",
      " -1.2232142e-03  9.9806273e-03 -2.2563811e-03 -4.7561764e-03\n",
      " -5.3302161e-03  6.9800834e-03 -5.7098186e-03  2.1140003e-03\n",
      " -5.2557373e-03  6.1218981e-03  4.3584020e-03  2.6075605e-03\n",
      " -1.4921337e-03 -2.7456607e-03  8.9923367e-03  5.2151373e-03\n",
      " -2.1627487e-03 -9.4697783e-03 -7.4261073e-03 -1.0637657e-03\n",
      " -7.9409441e-04 -2.5640149e-03  9.6830623e-03 -4.5818798e-04\n",
      "  5.8745639e-03 -7.4473573e-03 -2.5053592e-03 -5.5502281e-03]\n",
      "testexamplecom: [-0.00713889  0.00123997 -0.00717597 -0.00224507  0.00371884  0.00583346\n",
      "  0.00119794  0.00210324 -0.00411058  0.00722512 -0.0063073   0.00464721\n",
      " -0.00821983  0.00203702 -0.0049772  -0.00424844 -0.0031089   0.00565579\n",
      "  0.00579879 -0.00497534  0.0007726  -0.00849622  0.00781053  0.00925791\n",
      " -0.0027421   0.00080068  0.00074597  0.00547815 -0.00860632  0.00058523\n",
      "  0.00686961  0.00223227  0.00112505 -0.00932137  0.00848228 -0.00626401\n",
      " -0.00299353  0.00349244 -0.00077299  0.00141049  0.00178174 -0.00682872\n",
      " -0.00972514  0.00904023  0.00619715 -0.00691371  0.00340381  0.0002053\n",
      "  0.00475281 -0.00711995  0.00402699  0.00434629  0.0099565  -0.00447345\n",
      " -0.00138926 -0.00731766 -0.00969884 -0.00908087 -0.00102232 -0.00650336\n",
      "  0.00484918 -0.00616439  0.00251991  0.00073937 -0.00339313 -0.00097904\n",
      "  0.00997922  0.00914547 -0.00446263  0.00908297 -0.00564106  0.00593235\n",
      " -0.00309758  0.00343204  0.0030177   0.00690087 -0.00237367  0.00877446\n",
      "  0.00758859 -0.00954719 -0.00800784 -0.00763762  0.00292302 -0.00279326\n",
      " -0.00693054 -0.00812854  0.00830952  0.00198985 -0.00932827 -0.00479208\n",
      "  0.00313669 -0.00471338  0.00528077 -0.00423405  0.00264166 -0.0080453\n",
      "  0.00621116  0.00481837  0.00078798  0.00301342]\n",
      "email: [-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
      " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
      " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
      " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
      " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
      "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
      " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
      "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
      "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
      " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
      " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
      "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
      "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
      "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
      " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
      "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
      " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
      "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
      " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
      " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
      " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
      " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n",
      "via: [ 8.13477393e-03 -4.45946632e-03 -1.06516527e-03  1.00730662e-03\n",
      " -1.91646061e-04  1.14901690e-03  6.11530244e-03 -2.01187886e-05\n",
      " -3.24640819e-03 -1.51309627e-03  5.89697761e-03  1.51335867e-03\n",
      " -7.24223326e-04  9.33318771e-03 -4.92247939e-03 -8.38915061e-04\n",
      "  9.17755719e-03  6.75043557e-03  1.50226429e-03 -8.88595264e-03\n",
      "  1.14832271e-03 -2.28673220e-03  9.36954003e-03  1.20888639e-03\n",
      "  1.48833171e-03  2.40908680e-03 -1.83809246e-03 -4.99759754e-03\n",
      "  2.30625301e-04 -2.01129145e-03  6.59908820e-03  8.94199125e-03\n",
      " -6.74944022e-04  2.97568506e-03 -6.10965025e-03  1.69808872e-03\n",
      " -6.92909164e-03 -8.69612116e-03 -5.90234902e-03 -8.95664934e-03\n",
      "  7.28024635e-03 -5.77229308e-03  8.27661715e-03 -7.24356854e-03\n",
      "  3.42299673e-03  9.67331324e-03 -7.78676057e-03 -9.94741824e-03\n",
      " -4.32815123e-03 -2.68255151e-03 -2.72580743e-04 -8.83357972e-03\n",
      " -8.61600693e-03  2.79794680e-03 -8.20828043e-03 -9.06850584e-03\n",
      " -2.34325510e-03 -8.63146130e-03 -7.05612125e-03 -8.39955360e-03\n",
      " -3.00545304e-04 -4.56610322e-03  6.62781717e-03  1.52967684e-03\n",
      " -3.34181567e-03  6.11112872e-03 -6.01496827e-03 -4.65446059e-03\n",
      " -7.20967213e-03 -4.33825096e-03 -1.80803274e-03  6.49092346e-03\n",
      " -2.76903855e-03  4.91821067e-03  6.90407539e-03 -7.46388920e-03\n",
      "  4.56784386e-03  6.12539984e-03 -2.95509538e-03  6.62378361e-03\n",
      "  6.12380682e-03 -6.44351309e-03 -6.76735211e-03  2.54108151e-03\n",
      " -1.62424229e-03 -6.06632745e-03  9.50226467e-03 -5.12880553e-03\n",
      " -6.55331789e-03 -1.20153294e-04 -2.70087528e-03  4.43174620e-04\n",
      " -3.53898760e-03 -4.18800046e-04 -7.08738284e-04  8.22800328e-04\n",
      "  8.19624588e-03 -5.73761947e-03 -1.65788992e-03  5.57207456e-03]\n",
      "us: [ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
      " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
      " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
      "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
      "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
      "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
      " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
      " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
      " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
      " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
      "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
      "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
      "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
      "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
      " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
      "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
      "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
      " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
      "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
      "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
      " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
      "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
      "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
      " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
      " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n",
      "contact: [-9.5785465e-03  8.9431154e-03  4.1650687e-03  9.2347348e-03\n",
      "  6.6435025e-03  2.9247368e-03  9.8040197e-03 -4.4246409e-03\n",
      " -6.8033109e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n",
      "  9.7047603e-03 -3.5583067e-03  9.5494064e-03  8.3472609e-04\n",
      " -6.3384566e-03 -1.9771170e-03 -7.3770545e-03 -2.9795230e-03\n",
      "  1.0416972e-03  9.4826873e-03  9.3558477e-03 -6.5958775e-03\n",
      "  3.4751510e-03  2.2755705e-03 -2.4893521e-03 -9.2291720e-03\n",
      "  1.0271263e-03 -8.1657059e-03  6.3201892e-03 -5.8000805e-03\n",
      "  5.5354391e-03  9.8337233e-03 -1.6000033e-04  4.5284927e-03\n",
      " -1.8094003e-03  7.3607611e-03  3.9400971e-03 -9.0103243e-03\n",
      " -2.3985039e-03  3.6287690e-03 -9.9568366e-05 -1.2012708e-03\n",
      " -1.0554385e-03 -1.6716016e-03  6.0495257e-04  4.1650953e-03\n",
      " -4.2527914e-03 -3.8336217e-03 -5.2816868e-05  2.6935578e-04\n",
      " -1.6880632e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n",
      "  2.1035396e-03  6.6652300e-04  5.9696771e-03 -6.8423809e-03\n",
      " -6.8157101e-03 -4.4762576e-03  9.4358288e-03 -1.5918827e-03\n",
      " -9.4292425e-03 -5.4504158e-04 -4.4489228e-03  6.0000787e-03\n",
      " -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2498009e-03\n",
      "  5.9991982e-03  7.3973476e-03 -7.6214634e-03 -6.0530235e-03\n",
      " -6.8384409e-03 -7.9183402e-03 -9.4990805e-03 -2.1254970e-03\n",
      " -8.3593250e-04 -7.2562015e-03  6.7870365e-03  1.1196196e-03\n",
      "  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681297e-03\n",
      " -2.1766580e-03  4.3210792e-03 -5.0853146e-03  1.1307895e-03\n",
      "  2.8833640e-03 -1.5363609e-03  9.9322954e-03  8.3496347e-03\n",
      "  2.4156666e-03  7.1182456e-03  5.8914376e-03 -5.5806171e-03]\n",
      "at: [-0.00515624 -0.00666834 -0.00777684  0.00831073 -0.00198234 -0.00685496\n",
      " -0.00415439  0.00514413 -0.00286914 -0.00374966  0.00162143 -0.00277629\n",
      " -0.00158436  0.00107449 -0.00297794  0.00851928  0.00391094 -0.00995886\n",
      "  0.0062596  -0.00675425  0.00076943  0.00440423 -0.00510337 -0.00211067\n",
      "  0.00809548 -0.00424379 -0.00763626  0.00925791 -0.0021555  -0.00471943\n",
      "  0.0085708   0.00428334  0.00432484  0.00928451 -0.00845308  0.00525532\n",
      "  0.00203935  0.00418828  0.0016979   0.00446413  0.00448629  0.00610452\n",
      " -0.0032021  -0.00457573 -0.00042652  0.00253373 -0.00326317  0.00605772\n",
      "  0.00415413  0.00776459  0.00256927  0.00811668 -0.00138721  0.00807793\n",
      "  0.00371702 -0.00804732 -0.00393361 -0.00247188  0.00489304 -0.00087216\n",
      " -0.00283091  0.00783371  0.0093229  -0.00161493 -0.00515925 -0.00470176\n",
      " -0.00484605 -0.00960283  0.00137202 -0.00422492  0.00252671  0.00561448\n",
      " -0.00406591 -0.00959658  0.0015467  -0.00670012  0.00249517 -0.00378063\n",
      "  0.00707842  0.00064022  0.00356094 -0.00273913 -0.00171055  0.00765279\n",
      "  0.00140768 -0.00585045 -0.0078345   0.00123269  0.00645463  0.00555635\n",
      " -0.00897705  0.00859216  0.00404698  0.00746961  0.00974633 -0.00728958\n",
      " -0.00903996  0.005836    0.00939121  0.00350693]\n",
      "blog: [ 7.0906193e-03 -1.5702001e-03  7.9502836e-03 -9.4840815e-03\n",
      " -8.0332486e-03 -6.6406573e-03 -4.0032822e-03  4.9865372e-03\n",
      " -3.8179564e-03 -8.3207479e-03  8.4107248e-03 -3.7454066e-03\n",
      "  8.6126449e-03 -4.8959851e-03  3.9204694e-03  4.9229329e-03\n",
      "  2.3924010e-03 -2.8192576e-03  2.8456640e-03 -8.2586538e-03\n",
      " -2.7665340e-03 -2.5896605e-03  7.2492021e-03 -3.4672131e-03\n",
      " -6.6020442e-03  4.3415148e-03 -4.7521907e-04 -3.5954583e-03\n",
      "  6.8793590e-03  3.8756065e-03 -3.9012365e-03  7.7264453e-04\n",
      "  9.1435434e-03  7.7533363e-03  6.3627628e-03  4.6691392e-03\n",
      "  2.3824112e-03 -1.8418917e-03 -6.3741286e-03 -3.0132433e-04\n",
      " -1.5636905e-03 -5.7330925e-04 -6.2649576e-03  7.4358578e-03\n",
      " -6.5888353e-03 -7.2427522e-03 -2.7593828e-03 -1.5149538e-03\n",
      " -7.6319599e-03  6.9767999e-04 -5.3307102e-03 -1.2756108e-03\n",
      " -7.3621362e-03  1.9594999e-03  3.2723369e-03 -1.7524540e-05\n",
      " -5.4529295e-03 -1.7250710e-03  7.0852456e-03  3.7383125e-03\n",
      " -8.8763833e-03 -3.4129780e-03  2.3510573e-03  2.1401460e-03\n",
      " -9.4636949e-03  4.5723077e-03 -8.6611016e-03 -7.3829684e-03\n",
      "  3.4841951e-03 -3.4727308e-03  3.5661540e-03  8.8945003e-03\n",
      " -3.5739311e-03  9.3204100e-03  1.7094000e-03  9.8508922e-03\n",
      "  5.7062726e-03 -9.1516702e-03 -3.3262193e-03  6.5266783e-03\n",
      "  5.6020310e-03  8.7055126e-03  6.9233389e-03  8.0405213e-03\n",
      " -9.8230755e-03  4.2983387e-03 -5.0310646e-03  3.5161925e-03\n",
      "  6.0615619e-03  4.3926113e-03  7.5124209e-03  1.4962374e-03\n",
      " -1.2691752e-03  5.7708267e-03 -5.6411466e-03  3.9159200e-05\n",
      "  9.4550159e-03 -5.4845945e-03  3.8163927e-03 -8.1176097e-03]\n",
      "amazing: [ 9.7702928e-03  8.1651136e-03  1.2809718e-03  5.0975787e-03\n",
      "  1.4081288e-03 -6.4551616e-03 -1.4280510e-03  6.4491653e-03\n",
      " -4.6173059e-03 -3.9930656e-03  4.9244044e-03  2.7130984e-03\n",
      " -1.8479753e-03 -2.8769434e-03  6.0107317e-03 -5.7167388e-03\n",
      " -3.2367026e-03 -6.4878250e-03 -4.2346325e-03 -8.5809948e-03\n",
      " -4.4697891e-03 -8.5112294e-03  1.4037776e-03 -8.6181965e-03\n",
      " -9.9166557e-03 -8.2016252e-03 -6.7726658e-03  6.6805850e-03\n",
      "  3.7845564e-03  3.5616636e-04 -2.9579818e-03 -7.4283206e-03\n",
      "  5.3341867e-04  4.9989222e-04  1.9561886e-04  8.5259555e-04\n",
      "  7.8633073e-04 -6.8160298e-05 -8.0070542e-03 -5.8702733e-03\n",
      " -8.3829118e-03 -1.3120425e-03  1.8206370e-03  7.4171280e-03\n",
      " -1.9634271e-03 -2.3252917e-03  9.4871549e-03  7.9704521e-05\n",
      " -2.4045217e-03  8.6048469e-03  2.6870037e-03 -5.3439722e-03\n",
      "  6.5881060e-03  4.5101536e-03 -7.0544672e-03 -3.2317400e-04\n",
      "  8.3448651e-04  5.7473574e-03 -1.7176545e-03 -2.8065301e-03\n",
      "  1.7484308e-03  8.4717153e-04  1.1928272e-03 -2.6342822e-03\n",
      " -5.9857843e-03  7.3229838e-03  7.5873756e-03  8.2963575e-03\n",
      " -8.5988473e-03  2.6364254e-03 -3.5599626e-03  9.6204039e-03\n",
      "  2.9037679e-03  4.6411133e-03  2.3856151e-03  6.6084778e-03\n",
      " -5.7432903e-03  7.8944126e-03 -2.4109220e-03 -4.5618857e-03\n",
      " -2.0609903e-03  9.7335577e-03 -6.8565905e-03 -2.1917201e-03\n",
      "  7.0009995e-03 -5.5749417e-05 -6.2949671e-03 -6.3935257e-03\n",
      "  8.9403950e-03  6.4295758e-03  4.7735930e-03 -3.2620477e-03\n",
      " -9.2676198e-03  3.7868882e-03  7.1605504e-03 -5.6328895e-03\n",
      " -7.8650126e-03 -2.9727400e-03 -4.9318983e-03 -2.3151112e-03]\n",
      "this: [-1.9434640e-03 -5.2691204e-03  9.4496058e-03 -9.3008578e-03\n",
      "  4.5043458e-03  5.4050456e-03 -1.4092061e-03  9.0091061e-03\n",
      "  9.8868310e-03 -5.4768636e-03 -6.0213986e-03 -6.7484509e-03\n",
      " -7.8960154e-03 -3.0481727e-03 -5.5953451e-03 -8.3461516e-03\n",
      "  7.8397384e-04  2.9956226e-03  6.4161662e-03 -2.6307860e-03\n",
      " -4.4544507e-03  1.2496916e-03  3.9263113e-04  8.1183659e-03\n",
      "  1.8241795e-04  7.2336327e-03 -8.2664695e-03  8.4348796e-03\n",
      " -1.8890941e-03  8.7032039e-03 -7.6182820e-03  1.7975287e-03\n",
      "  1.0570943e-03  4.6461497e-05 -5.1045078e-03 -9.2492709e-03\n",
      " -7.2662327e-03 -7.9535712e-03  1.9131829e-03  4.7796028e-04\n",
      " -1.8128387e-03  7.1211252e-03 -2.4759402e-03 -1.3476524e-03\n",
      " -8.9022741e-03 -9.9272644e-03  8.9503666e-03 -5.7559991e-03\n",
      " -6.3747931e-03  5.2003399e-03  6.6708666e-03 -6.8337950e-03\n",
      "  9.5918431e-04 -6.0095917e-03  1.6471200e-03 -4.2905998e-03\n",
      " -3.4421731e-03  2.1854921e-03  8.6632241e-03  6.7292359e-03\n",
      " -9.6793231e-03 -5.6238398e-03  7.8824637e-03  1.9902042e-03\n",
      " -4.2576035e-03  5.9976627e-04  9.5218150e-03 -1.1034268e-03\n",
      " -9.4269626e-03  1.6081010e-03  6.2337616e-03  6.2845801e-03\n",
      "  4.0922244e-03 -5.6506260e-03 -3.7027901e-04 -5.5318025e-05\n",
      "  4.5735650e-03 -8.0433870e-03 -8.0202818e-03  2.6542690e-04\n",
      " -8.6095063e-03  5.8213174e-03 -4.1825301e-04  9.9738948e-03\n",
      " -5.3456584e-03 -4.8652000e-04  7.7589322e-03 -4.0687295e-03\n",
      " -5.0168992e-03  1.5905675e-03  2.6514751e-03 -2.5654770e-03\n",
      "  6.4483341e-03 -7.6611158e-03  3.3938743e-03  4.9011729e-04\n",
      "  8.7350197e-03  5.9831389e-03  6.8169381e-03  7.8240465e-03]\n",
      "out: [-0.00950012  0.00956222 -0.00777076 -0.00264551 -0.00490641 -0.0049667\n",
      " -0.00802359 -0.00778358 -0.00455321 -0.00127536 -0.00510299  0.00614054\n",
      " -0.00951662 -0.0053071   0.00943715  0.00699133  0.00767582  0.00423474\n",
      "  0.00050709 -0.00598114  0.00601878  0.00263503  0.00769943  0.00639384\n",
      "  0.00794257  0.00865741 -0.00989575 -0.0067557   0.00133757  0.0064403\n",
      "  0.00737382  0.00551698  0.00766163 -0.00512557  0.00658441 -0.00410837\n",
      " -0.00905534  0.00914168  0.0013314  -0.00275968 -0.00247784 -0.00422048\n",
      "  0.00481234  0.00440022 -0.00265336 -0.00734188 -0.00356585 -0.00033661\n",
      "  0.00609589 -0.00283734 -0.00012089  0.00087973 -0.00709565  0.002065\n",
      " -0.00143242  0.00280215  0.00484222 -0.00135202 -0.00278014  0.00773865\n",
      "  0.0050456   0.00671352  0.00451564  0.00866716  0.00747497 -0.00108189\n",
      "  0.00874764  0.00460172  0.00544063 -0.00138608 -0.00204132 -0.00442435\n",
      " -0.0085152   0.00303773  0.00888319  0.00891974 -0.00194235  0.00608616\n",
      "  0.00377972 -0.00429597  0.00204292 -0.00543789  0.00820889  0.00543291\n",
      "  0.00318443  0.00410257  0.00865715  0.00727203 -0.00083347 -0.00707277\n",
      "  0.00838047  0.00723358  0.00173047 -0.00134749 -0.00589009 -0.00453309\n",
      "  0.00864797 -0.00313511 -0.00633882  0.00987008]\n",
      "check: [ 7.7013010e-03  9.1194436e-03  1.1349504e-03 -8.3253728e-03\n",
      "  8.4257321e-03 -3.6960833e-03  5.7456801e-03  4.3919426e-03\n",
      "  9.6892621e-03 -9.2950417e-03  9.2122182e-03 -9.2816344e-03\n",
      " -6.9086943e-03 -9.0985103e-03 -5.5499845e-03  7.3692640e-03\n",
      "  9.1697788e-03 -3.3224283e-03  3.7241264e-03 -3.6297780e-03\n",
      "  7.8828335e-03  5.8662812e-03  4.7380813e-06 -3.6284532e-03\n",
      " -7.2242348e-03  4.7701388e-03  1.4523285e-03 -2.6159808e-03\n",
      "  7.8387428e-03 -4.0511186e-03 -9.1465637e-03 -2.2513927e-03\n",
      "  1.2482972e-04 -6.6384124e-03 -5.4900437e-03 -8.4996894e-03\n",
      "  9.2275133e-03  7.4206130e-03 -2.9809965e-04  7.3640151e-03\n",
      "  7.9550464e-03 -7.8649592e-04  6.6167754e-03  3.7643660e-03\n",
      "  5.0790166e-03  7.2585153e-03 -4.7436589e-03 -2.1904972e-03\n",
      "  8.7110489e-04  4.2352648e-03  3.3045195e-03  5.0921110e-03\n",
      "  4.5826812e-03 -8.4378682e-03 -3.1881072e-03 -7.2418847e-03\n",
      "  9.6813189e-03  5.0028553e-03  1.6735455e-04  4.1092001e-03\n",
      " -7.6569906e-03 -6.2974072e-03  3.0798628e-03  6.5359371e-03\n",
      "  3.9486620e-03  6.0215225e-03 -1.9892624e-03 -3.3477666e-03\n",
      "  2.0378825e-04 -3.1967482e-03 -5.5183936e-03 -7.7862237e-03\n",
      "  6.5347673e-03 -1.0879923e-03 -1.8876848e-03 -7.8091677e-03\n",
      "  9.3405982e-03  8.7130180e-04  1.7684299e-03  2.4951715e-03\n",
      " -7.3836190e-03  1.6358088e-03  2.9736343e-03 -8.5667390e-03\n",
      "  4.9555334e-03  2.4307237e-03  7.5031454e-03  5.0422722e-03\n",
      " -3.0352010e-03 -7.1636932e-03  7.0955558e-03  1.9019847e-03\n",
      "  5.1980233e-03  6.3815522e-03  1.9121042e-03 -6.1278078e-03\n",
      " -2.3894390e-06  8.2662962e-03 -6.1000334e-03  9.4418591e-03]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing URLs, special characters, and unnecessary whitespaces.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the cleaned text into words using Python's built-in methods.\n",
    "    \"\"\"\n",
    "    # Split text by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def train_word2vec(sentences, vector_size=100, window=5, min_count=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the provided sentences.\n",
    "\n",
    "    Parameters:\n",
    "        sentences (list of list of str): Tokenized sentences.\n",
    "        vector_size (int): Dimensionality of the word vectors.\n",
    "        window (int): Maximum distance between the current and predicted word.\n",
    "        min_count (int): Ignores all words with total frequency lower than this.\n",
    "\n",
    "    Returns:\n",
    "        Word2Vec: Trained Word2Vec model.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, sg=1)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    Check out this amazing blog at https://example.com!\n",
    "    Contact us via email: test@example.com or call +123-456-7890.\n",
    "    #Python #NLP :)\n",
    "    \"\"\"\n",
    "    print(\"Original Text:\\n\", sample_text)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(sample_text)\n",
    "    print(\"\\nCleaned Text:\\n\", cleaned_text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenize_text(cleaned_text)\n",
    "    print(\"\\nTokenized Text:\\n\", tokenized_text)\n",
    "\n",
    "    # Prepare data for Word2Vec (list of tokenized sentences)\n",
    "    sentences = [tokenized_text]  # List of sentences, each sentence is a list of tokens\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    word2vec_model = train_word2vec(sentences)\n",
    "    print(\"\\nWord2Vec Embeddings:\")\n",
    "    for word in word2vec_model.wv.index_to_key:\n",
    "        print(f\"{word}: {word2vec_model.wv[word]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "編碼 (Encoding) -- Word emgedding(GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing URLs, special characters, and unnecessary whitespaces.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the cleaned text into words using Python's built-in methods.\n",
    "    \"\"\"\n",
    "    # Split text by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def load_glove_embeddings(glove_file_path):\n",
    "    \"\"\"\n",
    "    Loads GloVe embeddings from a file.\n",
    "\n",
    "    Parameters:\n",
    "        glove_file_path (str): Path to the GloVe embeddings file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping words to their GloVe embeddings.\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefficients = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefficients\n",
    "    return embeddings_index\n",
    "\n",
    "def get_sentence_embedding(tokens, embeddings_index):\n",
    "    \"\"\"\n",
    "    Computes the sentence embedding by averaging GloVe embeddings of words.\n",
    "\n",
    "    Parameters:\n",
    "        tokens (list): List of tokens from the sentence.\n",
    "        embeddings_index (dict): GloVe word embeddings dictionary.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The sentence embedding as a vector.\n",
    "    \"\"\"\n",
    "    embedding_dim = len(next(iter(embeddings_index.values())))  # Dimension of GloVe embeddings\n",
    "    sentence_embedding = np.zeros(embedding_dim)\n",
    "    valid_tokens = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in embeddings_index:\n",
    "            sentence_embedding += embeddings_index[token]\n",
    "            valid_tokens += 1\n",
    "\n",
    "    if valid_tokens > 0:\n",
    "        sentence_embedding /= valid_tokens\n",
    "\n",
    "    return sentence_embedding\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    Check out this amazing blog at https://example.com!\n",
    "    Contact us via email: test@example.com or call +123-456-7890.\n",
    "    #Python #NLP :)\n",
    "    \"\"\"\n",
    "    print(\"Original Text:\\n\", sample_text)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(sample_text)\n",
    "    print(\"\\nCleaned Text:\\n\", cleaned_text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenize_text(cleaned_text)\n",
    "    print(\"\\nTokenized Text:\\n\", tokenized_text)\n",
    "\n",
    "    # Load GloVe embeddings (download GloVe file and provide the correct path)\n",
    "    glove_file_path = \"glove.6B.50d.txt\"  # Example file: 50-dimensional GloVe vectors\n",
    "    print(\"\\nLoading GloVe embeddings...\")\n",
    "    glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "    # Compute sentence embedding\n",
    "    sentence_embedding = get_sentence_embedding(tokenized_text, glove_embeddings)\n",
    "    print(\"\\nSentence Embedding:\\n\", sentence_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Step 2：**序列處理**\n",
    "1. **填充與截斷（Padding and Truncation）**：\n",
    "   - 確保所有序列的長度一致，短序列用填充符（如 0）補齊，長序列則截斷。\n",
    "2. **批量處理（Batch Preparation）**：\n",
    "   - 將資料集劃分為訓練集、驗證集和測試集，並生成小批量資料以提高計算效率。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3：**RNN 模型架構**\n",
    "1. **輸入層（Input Layer）**：\n",
    "   - 接收數字化表示的序列資料。\n",
    "2. **嵌入層（Embedding Layer）**：\n",
    "   - 如果未使用預訓練嵌入，則將輸入數字轉換為密集向量表示。\n",
    "3. **RNN 層**：\n",
    "   - 負責處理序列資料。常見變體包括：\n",
    "     - 簡單 RNN\n",
    "     - 長短期記憶（LSTM）網絡\n",
    "     - 閘門循環單元（GRU）。\n",
    "4. **全連接層（Dense Layer）**：\n",
    "   - 將 RNN 的輸出映射到目標維度（例如分類任務中的類別數）。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4：**損失函數**\n",
    "- 根據任務選擇適合的損失函數：\n",
    "  - **交叉熵損失（Cross-Entropy Loss）**：用於分類任務。\n",
    "  - **均方誤差（Mean Squared Error）**：用於回歸任務。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5：**優化器選擇**\n",
    "- 使用如 Adam、SGD 或 RMSProp 等優化器，幫助最小化損失。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6：**模型訓練**\n",
    "1. **前向傳播（Forward Propagation）**：\n",
    "   - 將輸入資料通過 RNN，生成預測結果。\n",
    "2. **時間反向傳播（BPTT，Backpropagation Through Time）**：\n",
    "   - 計算序列的梯度並更新權重。\n",
    "3. **多次訓練（Epochs）**：\n",
    "   - 對多個小批量進行多輪訓練以保證模型收斂。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7：**模型評估**\n",
    "- 使用驗證資料進行評估。\n",
    "- 根據任務需求，評估指標如準確率（Accuracy）、F1分數、或 BLEU 分數（機器翻譯）等。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 8：**模型微調**\n",
    "- 調整超參數，如學習率、Dropout 比例、RNN 層數等。\n",
    "- 如果使用了預訓練嵌入，可對特定任務進行微調。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 9：**部署**\n",
    "- 保存訓練好的模型。\n",
    "- 在實際應用中進行推理，如文本分類、情感分析或翻譯。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('', 0), ('[UNK]', 1), ('i', 2), ('write', 3), ('erase', 4), ('rewrite', 5), ('again', 6), ('and', 7), ('then', 8), ('a', 9), ('poppy', 10), ('blooms', 11)])\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1 \n",
    "    return vector\n",
    "\n",
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text \n",
    "                        if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [           \n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)\n",
    "print( vectorizer.vocabulary.items() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7, 1, 5, 6]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1 \n",
    "    return vector\n",
    "\n",
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text \n",
    "                        if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [           \n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)\n",
    "\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write rewrite and [UNK] rewrite again'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1 \n",
    "    return vector\n",
    "\n",
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text \n",
    "                        if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [           \n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)\n",
    "\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "\n",
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AclImdb dataset\n",
    "\n",
    "prepare a validation set by setting apart 20% of the training text files in a new directory, `aclImdb/val`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset\n",
    "The Stanford IMDb dataset is available in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Load the dataset\n",
    "max_features = 10000  # Vocabulary size\n",
    "max_len = 500         # Maximum length of sequences\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences to make them of equal length\n",
    "train_data = pad_sequences(train_data, maxlen=max_len)\n",
    "test_data = pad_sequences(test_data, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the RNN Model\n",
    "An RNN model is suitable for sequence data like text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/113_Python/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-12-03 14:41:11.850950: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-12-03 14:41:11.850982: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-12-03 14:41:11.850990: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-12-03 14:41:11.851010: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-03 14:41:11.851024: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=128, input_length=max_len),\n",
    "    SimpleRNN(128, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the RNN Model\n",
    "Use the training data to train the RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 14:41:24.006670: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-12-03 14:41:24.020396: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  6/313\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11:05:40\u001b[0m 130s/step - accuracy: 0.4589 - loss: 0.7181"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_data, \n",
    "    train_labels, \n",
    "    epochs=5, \n",
    "    batch_size=64, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Model\n",
    "Test the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize the Training Process\n",
    "You can visualize the accuracy and loss over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# following parts just for test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"../data/aclImdb\")\n",
    "train_dir = base_dir / \"train\"\n",
    "val_dir = base_dir / \"val\"\n",
    "for category in ( \"neg\", \"pos\" ):\n",
    "    if not( ( pathlib.Path.cwd() / val_dir / category ).exists() ):\n",
    "        os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, \n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing groups of words\n",
    "\n",
    "`following code run one time only.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70000 files belonging to 3 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"../data/aclImdb\")\n",
    "train_dir = base_dir / \"train\"\n",
    "val_dir = base_dir / \"val\"\n",
    "test_dir = base_dir / \"test\"\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    train_dir, batch_size=batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    val_dir, batch_size=batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    test_dir, batch_size=batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.target: <dtype: 'int32'>\n",
      "inputs[0]: b'Dear Friends and Family,<br /><br />I guess if one teen wants to become biblical with another teen, then that\\'s their eternal damnation - just remember kids, \"birth control\" doesn\\'t mean \"oral sex\", I don\\'t care what the honor student says. On the other hand, even if the senator\\'s aid quotes himself as a \"bit of a romantic guy\", he\\'s still only hitting on a high school girl. If she was my sister, I\\'d eat this guys kneecaps.<br /><br />Other than that I found out that Mongolians don\\'t kiss the same way the French do and that baseball players named Zoo like delicate undergarments.<br /><br />I think I\\'d almost rather watch Richie Rich one more time than suffer the indignity of this slip, slap, slop. Thank you, and good night.'\n",
      "targets[0]: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 14:08:55.786257: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print( f\"inputs.shape: {inputs.shape}\" )\n",
    "    print( f\"inputs.dtype: {inputs.dtype}\" )\n",
    "    print( f\"targets.shape: {targets.shape}\" )\n",
    "    print( f\"targets.target: {targets.dtype}\" )\n",
    "    print( f\"inputs[0]: {inputs[0]}\" )\n",
    "    print( f\"targets[0]: {targets[0]}\" )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 14:15:06.144300: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Cannot batch tensors with different shapes in component 0. First element had shape [32] and element 11 had shape [16].\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 0. First element had shape [32] and element 11 had shape [16]. [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m text_only_train_ds \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: tf\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Adapt the vectorization layer\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtext_vectorization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_only_train_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Apply TextVectorization to datasets\u001b[39;00m\n\u001b[1;32m     21\u001b[0m binary_1gram_train_ds \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x, y: (text_vectorization(tf\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), y),\n\u001b[1;32m     23\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/113_Python/lib/python3.11/site-packages/keras/src/layers/preprocessing/text_vectorization.py:420\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtake(steps)\n\u001b[0;32m--> 420\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/113_Python/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:809\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    808\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/113_Python/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:772\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 772\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/113_Python/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3086\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3085\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3086\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3088\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/113_Python/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 0. First element had shape [32] and element 11 had shape [16]. [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the TextVectorization layer\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000, output_sequence_length=500\n",
    ")\n",
    "\n",
    "# Prepare datasets (ensure proper batching)\n",
    "train_ds = train_ds.batch(32).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(32).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(32).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Fix shape by expanding dimensions\n",
    "text_only_train_ds = train_ds.map(lambda x, y: tf.expand_dims(x, axis=-1))\n",
    "\n",
    "# Adapt the vectorization layer\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# Apply TextVectorization to datasets\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(tf.expand_dims(x, axis=-1)), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(tf.expand_dims(x, axis=-1)), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(tf.expand_dims(x, axis=-1)), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.target: <dtype: 'int32'>\n",
      "inputs[0]: b'As far as serial- killer films and thrillers are concerned this one\\'s right down there with the worst of \\'em. Only \"Copycat\" and \"Sisters\" manage to be more annoying in their absurdity, and only \"Saw\" is worse. And it\\'s a pity because the first hour is genuinely eerie, with a fittingly claustrophobic mortuary as the ideal setting. I am a very jaded viewer, but some of those early mortuary scenes really get under your skin. However, after an hour it all goes downhill, and I mean steeply. The relentlessly stupid plot twists rob you of all patience and the movie just keeps sinking to new lows. All logic is thrown into the wind, and the viewer\\'s intelligence is insulted and pounded upon repeatedly with more force than that baseball bat could ever have generated - the one used by Nolte and McGregor. (I\\'d be the first to sign up if they were looking for volunteers to take that baseball bat and bash the heads of the writers of this nonsense.) Nothing here adds up. Absolutely nothing. Nolte was molesting corpses decades earlier and the writers of this film would have us believe that this man could years later become the city\\'s chief police investigator! Making him the killer is as absurd as giving Thomas Edison credit for inventing the wheel, as laughable as a conspiracy-theory plot from the \"X-Files\", and as stupid as Kim Basinger\\'s book on how to solve all of world\\'s problems (if she\\'d ever write one). The very notion that a man - so disturbed that he indulges in necrophilia in mortuaries - would have the sanity, interest, patience, and willingness to climb all the way to chief investigator in a police department only to start a savage murder spree is simply a mind-bogglingly dumb, far-fetched concept to me.<br /><br />And how the hell did he even start with the framing of McGregor? This is an essential piece of the puzzle that is badly missing here; McGregor JUST HAPPENS to get a job where he meets Nolte. And McGregor\\'s best friend, Brolin, JUST HAPPENS to know a prostitute who is Nolte\\'s play toy (and later victim). It can sure be a small, small world in a Hollywood stinker! And to add some silliness, Brolin is some kind of a deranged thrill-seeker who acts like a total lunatic all the time. Obviously, he is the decoy for the viewer; we are meant to treat him as the suspect. But how dumb do they think we are? And how the hell did Brolin get into the mortuary when he carried out his \"practical joke\"? And how the hell did Nolte manage to drag out a body of one of his victims within seconds of McGregor entering the room, without McGregor noticing it (the fact that he had his walkman on and/or was singing and/or talking doesn\\'t make it any more believable)? And what\\'s with this annoying scene where Arquette JUST HAPPENS to walk into the mortuary at exactly the moment when McGregor is hitting Nolte with a baseball bat and predictably starts thinking her boyfriend is the killer??! More annoyances came from the scene in which Brolin\\'s reaction to McGregor\\'s telling him that the latter is been suspected of murder is to laugh! Or the one in which he cuts off his own thumb in order to free himself and save the others. Sure,... why not?? (\"I am being hand-cuffed to this metal pole, and as a result can neither save my friends nor myself... What do I do...? I know!... I\\'ll cut my thumb off! How come I didn\\'t think of that before!?...\") Nolte makes the best out of his role, but due to the bitchingly silly script he appears to be hamming it up too much at the end - but what choice did he have? McGregor is solid, too, apart from his on-and-off accent (which was it now? American or English?).'\n",
      "targets[0]: 2\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print( f\"inputs.shape: {inputs.shape}\" )\n",
    "    print( f\"inputs.dtype: {inputs.dtype}\" )\n",
    "    print( f\"targets.shape: {targets.shape}\" )\n",
    "    print( f\"targets.target: {targets.dtype}\" )\n",
    "    print( f\"inputs[0]: {inputs[0]}\" )\n",
    "    print( f\"targets[0]: {targets[0]}\" )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100006 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:36:43.988070: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text \n",
    "                        if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "dataset = keras.utils.text_dataset_from_directory(\n",
    "    directory=\"../data/aclImdb\",\n",
    "    label_mode=None,\n",
    "    batch_size=256\n",
    ")\n",
    "dataset = dataset.map( lambda x: tf.strings.regex_replace( x, \"<br />\", \" \") )\n",
    "vectorizer = Vectorizer()\n",
    "vocabulary = {}\n",
    "for text_batch in dataset:\n",
    "    for text in text_batch.numpy():  # Ensure to iterate over batch\n",
    "        if isinstance(text, bytes):  # Check if it's a byte string\n",
    "            text = text.decode('utf-8')  # Decode only if necessary\n",
    "        standardized_text = vectorizer.standardize(text)\n",
    "        tokens = vectorizer.tokenize(standardized_text)\n",
    "        for token in tokens:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = len(vocabulary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "113_Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
