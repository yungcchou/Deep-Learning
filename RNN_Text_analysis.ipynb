{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用循環神經網絡（RNN）自然語言處理（NLP）\n",
    "\n",
    "### Step 1：**資料準備**\n",
    "1. **資料清理**：\n",
    "   - 移除雜訊，如特殊字符、URL、不必要的空格等。\n",
    "2. **分詞（Tokenization）**：\n",
    "   - 將文本分割成詞或子詞（如 WordPiece、BPE）。\n",
    "3. **編碼（Encoding）**：\n",
    "   - 將分詞結果轉換為數字表示。\n",
    "   - 常見的方法包括：\n",
    "     - One-hot 編碼\n",
    "     - 詞嵌入（如 Word2Vec、GloVe）\n",
    "     - 預訓練嵌入（如 BERT 或 GPT）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "    Check out this amazing blog at https://example.com!\n",
      "    Contact us via email: test@example.com or call +123-456-7890.\n",
      "    #Python #NLP :)\n",
      "    \n",
      "\n",
      "Cleaned Text:\n",
      " check out this amazing blog at contact us via email testexamplecom or call python nlp\n"
     ]
    }
   ],
   "source": [
    "# 資料清理\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    Check out this amazing blog at https://example.com!\n",
    "    Contact us via email: test@example.com or call +123-456-7890.\n",
    "    #Python #NLP :)\n",
    "    \"\"\"\n",
    "    cleaned_text = clean_text(sample_text)\n",
    "    print(\"Original Text:\\n\", sample_text)\n",
    "    print(\"\\nCleaned Text:\\n\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分詞（Tokenization）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "    Check out this amazing blog at https://example.com!\n",
      "    Contact us via email: test@example.com or call +123-456-7890.\n",
      "    #Python #NLP :)\n",
      "    \n",
      "\n",
      "Cleaned Text:\n",
      " check out this amazing blog at contact us via email testexamplecom or call python nlp\n",
      "\n",
      "Tokenized Text:\n",
      " ['check', 'out', 'this', 'amazing', 'blog', 'at', 'contact', 'us', 'via', 'email', 'testexamplecom', 'or', 'call', 'python', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing URLs, special characters, and unnecessary whitespaces.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the cleaned text into words using Python's built-in methods.\n",
    "    \"\"\"\n",
    "    # Split text by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    Check out this amazing blog at https://example.com!\n",
    "    Contact us via email: test@example.com or call +123-456-7890.\n",
    "    #Python #NLP :)\n",
    "    \"\"\"\n",
    "    print(\"Original Text:\\n\", sample_text)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(sample_text)\n",
    "    print(\"\\nCleaned Text:\\n\", cleaned_text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenize_text(cleaned_text)\n",
    "    print(\"\\nTokenized Text:\\n\", tokenized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 編碼（Encoding）-- One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "    Check out this amazing blog at https://example.com!\n",
      "    Contact us via email: test@example.com or call +123-456-7890.\n",
      "    #Python #NLP :)\n",
      "    \n",
      "\n",
      "Cleaned Text:\n",
      " check out this amazing blog at contact us via email testexamplecom or call python nlp\n",
      "\n",
      "Tokenized Text:\n",
      " ['check', 'out', 'this', 'amazing', 'blog', 'at', 'contact', 'us', 'via', 'email', 'testexamplecom', 'or', 'call', 'python', 'nlp']\n",
      "\n",
      "Word to Index Mapping:\n",
      " {'amazing': 0, 'at': 1, 'blog': 2, 'call': 3, 'check': 4, 'contact': 5, 'email': 6, 'nlp': 7, 'or': 8, 'out': 9, 'python': 10, 'testexamplecom': 11, 'this': 12, 'us': 13, 'via': 14}\n",
      "\n",
      "One-Hot Encoded Vectors:\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing URLs, special characters, and unnecessary whitespaces.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the cleaned text into words using Python's built-in methods.\n",
    "    \"\"\"\n",
    "    # Split text by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def one_hot_encode(tokens):\n",
    "    \"\"\"\n",
    "    Converts a list of tokens into a one-hot encoded representation.\n",
    "    \n",
    "    Parameters:\n",
    "        tokens (list): List of tokens from tokenized text.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A dictionary mapping words to indices and a one-hot encoded list of lists.\n",
    "    \"\"\"\n",
    "    # Create a vocabulary of unique tokens\n",
    "    vocabulary = sorted(set(tokens))\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    \n",
    "    # Generate one-hot encoding for each token\n",
    "    one_hot_vectors = []\n",
    "    for token in tokens:\n",
    "        one_hot_vector = [0] * len(vocabulary)\n",
    "        one_hot_vector[word_to_index[token]] = 1\n",
    "        one_hot_vectors.append(one_hot_vector)\n",
    "    \n",
    "    return word_to_index, one_hot_vectors\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    Check out this amazing blog at https://example.com!\n",
    "    Contact us via email: test@example.com or call +123-456-7890.\n",
    "    #Python #NLP :)\n",
    "    \"\"\"\n",
    "    print(\"Original Text:\\n\", sample_text)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(sample_text)\n",
    "    print(\"\\nCleaned Text:\\n\", cleaned_text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenize_text(cleaned_text)\n",
    "    print(\"\\nTokenized Text:\\n\", tokenized_text)\n",
    "\n",
    "    # One-hot encode the tokens\n",
    "    word_to_index, one_hot_vectors = one_hot_encode(tokenized_text)\n",
    "    print(\"\\nWord to Index Mapping:\\n\", word_to_index)\n",
    "    print(\"\\nOne-Hot Encoded Vectors:\")\n",
    "    for vector in one_hot_vectors:\n",
    "        print(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 編碼 (Encoding) -- 詞嵌入（如 Word2Vec、GloVe）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "    Check out this amazing blog at https://example.com!\n",
      "    Contact us via email: test@example.com or call +123-456-7890.\n",
      "    #Python #NLP :)\n",
      "    \n",
      "\n",
      "Cleaned Text:\n",
      " check out this amazing blog at contact us via email testexamplecom or call python nlp\n",
      "\n",
      "Tokenized Text:\n",
      " ['check', 'out', 'this', 'amazing', 'blog', 'at', 'contact', 'us', 'via', 'email', 'testexamplecom', 'or', 'call', 'python', 'nlp']\n",
      "\n",
      "Word2Vec Embeddings:\n",
      "nlp: [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
      "python: [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
      "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
      " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
      "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
      "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
      " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
      " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
      " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
      " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
      "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
      " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
      "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
      "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
      " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
      "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
      "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
      "call: [ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      "or: [-8.2440982e-03  9.2991991e-03 -1.9725331e-04 -1.9685079e-03\n",
      "  4.6036434e-03 -4.0951595e-03  2.7420460e-03  6.9410047e-03\n",
      "  6.0661924e-03 -7.5107221e-03  9.3817133e-03  4.6716975e-03\n",
      "  3.9660614e-03 -6.2439428e-03  8.4604742e-03 -2.1511274e-03\n",
      "  8.8242898e-03 -5.3622089e-03 -8.1289113e-03  6.8254727e-03\n",
      "  1.6703474e-03 -2.1991611e-03  9.5135355e-03  9.4952369e-03\n",
      " -9.7737284e-03  2.5051939e-03  6.1564865e-03  3.8730367e-03\n",
      "  2.0229670e-03  4.3106405e-04  6.7346974e-04 -3.8211411e-03\n",
      " -7.1399156e-03 -2.0880306e-03  3.9246641e-03  8.8188052e-03\n",
      "  9.2591848e-03 -5.9765638e-03 -9.4021866e-03  9.7645782e-03\n",
      "  3.4283167e-03  5.1672393e-03  6.2813754e-03 -2.8041822e-03\n",
      "  7.3210048e-03  2.8288446e-03  2.8727155e-03 -2.3800777e-03\n",
      " -3.1294629e-03 -2.3699196e-03  4.2772293e-03  7.5820331e-05\n",
      " -9.5850844e-03 -9.6653933e-03 -6.1471672e-03 -1.2862809e-04\n",
      "  1.9972187e-03  9.4322599e-03  5.5857045e-03 -4.2903535e-03\n",
      "  2.7716716e-04  4.9646846e-03  7.6989033e-03 -1.1450007e-03\n",
      "  4.3227156e-03 -5.8153337e-03 -8.0281531e-04  8.0997068e-03\n",
      " -2.3602233e-03 -9.6629262e-03  5.7801716e-03 -3.9290586e-03\n",
      " -1.2232142e-03  9.9806273e-03 -2.2563811e-03 -4.7561764e-03\n",
      " -5.3302161e-03  6.9800834e-03 -5.7098186e-03  2.1140003e-03\n",
      " -5.2557373e-03  6.1218981e-03  4.3584020e-03  2.6075605e-03\n",
      " -1.4921337e-03 -2.7456607e-03  8.9923367e-03  5.2151373e-03\n",
      " -2.1627487e-03 -9.4697783e-03 -7.4261073e-03 -1.0637657e-03\n",
      " -7.9409441e-04 -2.5640149e-03  9.6830623e-03 -4.5818798e-04\n",
      "  5.8745639e-03 -7.4473573e-03 -2.5053592e-03 -5.5502281e-03]\n",
      "testexamplecom: [-0.00713889  0.00123997 -0.00717597 -0.00224507  0.00371884  0.00583346\n",
      "  0.00119794  0.00210324 -0.00411058  0.00722512 -0.0063073   0.00464721\n",
      " -0.00821983  0.00203702 -0.0049772  -0.00424844 -0.0031089   0.00565579\n",
      "  0.00579879 -0.00497534  0.0007726  -0.00849622  0.00781053  0.00925791\n",
      " -0.0027421   0.00080068  0.00074597  0.00547815 -0.00860632  0.00058523\n",
      "  0.00686961  0.00223227  0.00112505 -0.00932137  0.00848228 -0.00626401\n",
      " -0.00299353  0.00349244 -0.00077299  0.00141049  0.00178174 -0.00682872\n",
      " -0.00972514  0.00904023  0.00619715 -0.00691371  0.00340381  0.0002053\n",
      "  0.00475281 -0.00711995  0.00402699  0.00434629  0.0099565  -0.00447345\n",
      " -0.00138926 -0.00731766 -0.00969884 -0.00908087 -0.00102232 -0.00650336\n",
      "  0.00484918 -0.00616439  0.00251991  0.00073937 -0.00339313 -0.00097904\n",
      "  0.00997922  0.00914547 -0.00446263  0.00908297 -0.00564106  0.00593235\n",
      " -0.00309758  0.00343204  0.0030177   0.00690087 -0.00237367  0.00877446\n",
      "  0.00758859 -0.00954719 -0.00800784 -0.00763762  0.00292302 -0.00279326\n",
      " -0.00693054 -0.00812854  0.00830952  0.00198985 -0.00932827 -0.00479208\n",
      "  0.00313669 -0.00471338  0.00528077 -0.00423405  0.00264166 -0.0080453\n",
      "  0.00621116  0.00481837  0.00078798  0.00301342]\n",
      "email: [-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
      " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
      " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
      " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
      " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
      "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
      " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
      "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
      "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
      " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
      " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
      "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
      "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
      "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
      " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
      "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
      " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
      "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
      " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
      " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
      " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
      " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n",
      "via: [ 8.13477393e-03 -4.45946632e-03 -1.06516527e-03  1.00730662e-03\n",
      " -1.91646061e-04  1.14901690e-03  6.11530244e-03 -2.01187886e-05\n",
      " -3.24640819e-03 -1.51309627e-03  5.89697761e-03  1.51335867e-03\n",
      " -7.24223326e-04  9.33318771e-03 -4.92247939e-03 -8.38915061e-04\n",
      "  9.17755719e-03  6.75043557e-03  1.50226429e-03 -8.88595264e-03\n",
      "  1.14832271e-03 -2.28673220e-03  9.36954003e-03  1.20888639e-03\n",
      "  1.48833171e-03  2.40908680e-03 -1.83809246e-03 -4.99759754e-03\n",
      "  2.30625301e-04 -2.01129145e-03  6.59908820e-03  8.94199125e-03\n",
      " -6.74944022e-04  2.97568506e-03 -6.10965025e-03  1.69808872e-03\n",
      " -6.92909164e-03 -8.69612116e-03 -5.90234902e-03 -8.95664934e-03\n",
      "  7.28024635e-03 -5.77229308e-03  8.27661715e-03 -7.24356854e-03\n",
      "  3.42299673e-03  9.67331324e-03 -7.78676057e-03 -9.94741824e-03\n",
      " -4.32815123e-03 -2.68255151e-03 -2.72580743e-04 -8.83357972e-03\n",
      " -8.61600693e-03  2.79794680e-03 -8.20828043e-03 -9.06850584e-03\n",
      " -2.34325510e-03 -8.63146130e-03 -7.05612125e-03 -8.39955360e-03\n",
      " -3.00545304e-04 -4.56610322e-03  6.62781717e-03  1.52967684e-03\n",
      " -3.34181567e-03  6.11112872e-03 -6.01496827e-03 -4.65446059e-03\n",
      " -7.20967213e-03 -4.33825096e-03 -1.80803274e-03  6.49092346e-03\n",
      " -2.76903855e-03  4.91821067e-03  6.90407539e-03 -7.46388920e-03\n",
      "  4.56784386e-03  6.12539984e-03 -2.95509538e-03  6.62378361e-03\n",
      "  6.12380682e-03 -6.44351309e-03 -6.76735211e-03  2.54108151e-03\n",
      " -1.62424229e-03 -6.06632745e-03  9.50226467e-03 -5.12880553e-03\n",
      " -6.55331789e-03 -1.20153294e-04 -2.70087528e-03  4.43174620e-04\n",
      " -3.53898760e-03 -4.18800046e-04 -7.08738284e-04  8.22800328e-04\n",
      "  8.19624588e-03 -5.73761947e-03 -1.65788992e-03  5.57207456e-03]\n",
      "us: [ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
      " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
      " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
      "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
      "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
      "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
      " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
      " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
      " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
      " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
      "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
      "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
      "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
      "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
      " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
      "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
      "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
      " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
      "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
      "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
      " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
      "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
      "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
      " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
      " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n",
      "contact: [-9.5785465e-03  8.9431154e-03  4.1650687e-03  9.2347348e-03\n",
      "  6.6435025e-03  2.9247368e-03  9.8040197e-03 -4.4246409e-03\n",
      " -6.8033109e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n",
      "  9.7047603e-03 -3.5583067e-03  9.5494064e-03  8.3472609e-04\n",
      " -6.3384566e-03 -1.9771170e-03 -7.3770545e-03 -2.9795230e-03\n",
      "  1.0416972e-03  9.4826873e-03  9.3558477e-03 -6.5958775e-03\n",
      "  3.4751510e-03  2.2755705e-03 -2.4893521e-03 -9.2291720e-03\n",
      "  1.0271263e-03 -8.1657059e-03  6.3201892e-03 -5.8000805e-03\n",
      "  5.5354391e-03  9.8337233e-03 -1.6000033e-04  4.5284927e-03\n",
      " -1.8094003e-03  7.3607611e-03  3.9400971e-03 -9.0103243e-03\n",
      " -2.3985039e-03  3.6287690e-03 -9.9568366e-05 -1.2012708e-03\n",
      " -1.0554385e-03 -1.6716016e-03  6.0495257e-04  4.1650953e-03\n",
      " -4.2527914e-03 -3.8336217e-03 -5.2816868e-05  2.6935578e-04\n",
      " -1.6880632e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n",
      "  2.1035396e-03  6.6652300e-04  5.9696771e-03 -6.8423809e-03\n",
      " -6.8157101e-03 -4.4762576e-03  9.4358288e-03 -1.5918827e-03\n",
      " -9.4292425e-03 -5.4504158e-04 -4.4489228e-03  6.0000787e-03\n",
      " -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2498009e-03\n",
      "  5.9991982e-03  7.3973476e-03 -7.6214634e-03 -6.0530235e-03\n",
      " -6.8384409e-03 -7.9183402e-03 -9.4990805e-03 -2.1254970e-03\n",
      " -8.3593250e-04 -7.2562015e-03  6.7870365e-03  1.1196196e-03\n",
      "  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681297e-03\n",
      " -2.1766580e-03  4.3210792e-03 -5.0853146e-03  1.1307895e-03\n",
      "  2.8833640e-03 -1.5363609e-03  9.9322954e-03  8.3496347e-03\n",
      "  2.4156666e-03  7.1182456e-03  5.8914376e-03 -5.5806171e-03]\n",
      "at: [-0.00515624 -0.00666834 -0.00777684  0.00831073 -0.00198234 -0.00685496\n",
      " -0.00415439  0.00514413 -0.00286914 -0.00374966  0.00162143 -0.00277629\n",
      " -0.00158436  0.00107449 -0.00297794  0.00851928  0.00391094 -0.00995886\n",
      "  0.0062596  -0.00675425  0.00076943  0.00440423 -0.00510337 -0.00211067\n",
      "  0.00809548 -0.00424379 -0.00763626  0.00925791 -0.0021555  -0.00471943\n",
      "  0.0085708   0.00428334  0.00432484  0.00928451 -0.00845308  0.00525532\n",
      "  0.00203935  0.00418828  0.0016979   0.00446413  0.00448629  0.00610452\n",
      " -0.0032021  -0.00457573 -0.00042652  0.00253373 -0.00326317  0.00605772\n",
      "  0.00415413  0.00776459  0.00256927  0.00811668 -0.00138721  0.00807793\n",
      "  0.00371702 -0.00804732 -0.00393361 -0.00247188  0.00489304 -0.00087216\n",
      " -0.00283091  0.00783371  0.0093229  -0.00161493 -0.00515925 -0.00470176\n",
      " -0.00484605 -0.00960283  0.00137202 -0.00422492  0.00252671  0.00561448\n",
      " -0.00406591 -0.00959658  0.0015467  -0.00670012  0.00249517 -0.00378063\n",
      "  0.00707842  0.00064022  0.00356094 -0.00273913 -0.00171055  0.00765279\n",
      "  0.00140768 -0.00585045 -0.0078345   0.00123269  0.00645463  0.00555635\n",
      " -0.00897705  0.00859216  0.00404698  0.00746961  0.00974633 -0.00728958\n",
      " -0.00903996  0.005836    0.00939121  0.00350693]\n",
      "blog: [ 7.0906193e-03 -1.5702001e-03  7.9502836e-03 -9.4840815e-03\n",
      " -8.0332486e-03 -6.6406573e-03 -4.0032822e-03  4.9865372e-03\n",
      " -3.8179564e-03 -8.3207479e-03  8.4107248e-03 -3.7454066e-03\n",
      "  8.6126449e-03 -4.8959851e-03  3.9204694e-03  4.9229329e-03\n",
      "  2.3924010e-03 -2.8192576e-03  2.8456640e-03 -8.2586538e-03\n",
      " -2.7665340e-03 -2.5896605e-03  7.2492021e-03 -3.4672131e-03\n",
      " -6.6020442e-03  4.3415148e-03 -4.7521907e-04 -3.5954583e-03\n",
      "  6.8793590e-03  3.8756065e-03 -3.9012365e-03  7.7264453e-04\n",
      "  9.1435434e-03  7.7533363e-03  6.3627628e-03  4.6691392e-03\n",
      "  2.3824112e-03 -1.8418917e-03 -6.3741286e-03 -3.0132433e-04\n",
      " -1.5636905e-03 -5.7330925e-04 -6.2649576e-03  7.4358578e-03\n",
      " -6.5888353e-03 -7.2427522e-03 -2.7593828e-03 -1.5149538e-03\n",
      " -7.6319599e-03  6.9767999e-04 -5.3307102e-03 -1.2756108e-03\n",
      " -7.3621362e-03  1.9594999e-03  3.2723369e-03 -1.7524540e-05\n",
      " -5.4529295e-03 -1.7250710e-03  7.0852456e-03  3.7383125e-03\n",
      " -8.8763833e-03 -3.4129780e-03  2.3510573e-03  2.1401460e-03\n",
      " -9.4636949e-03  4.5723077e-03 -8.6611016e-03 -7.3829684e-03\n",
      "  3.4841951e-03 -3.4727308e-03  3.5661540e-03  8.8945003e-03\n",
      " -3.5739311e-03  9.3204100e-03  1.7094000e-03  9.8508922e-03\n",
      "  5.7062726e-03 -9.1516702e-03 -3.3262193e-03  6.5266783e-03\n",
      "  5.6020310e-03  8.7055126e-03  6.9233389e-03  8.0405213e-03\n",
      " -9.8230755e-03  4.2983387e-03 -5.0310646e-03  3.5161925e-03\n",
      "  6.0615619e-03  4.3926113e-03  7.5124209e-03  1.4962374e-03\n",
      " -1.2691752e-03  5.7708267e-03 -5.6411466e-03  3.9159200e-05\n",
      "  9.4550159e-03 -5.4845945e-03  3.8163927e-03 -8.1176097e-03]\n",
      "amazing: [ 9.7702928e-03  8.1651136e-03  1.2809718e-03  5.0975787e-03\n",
      "  1.4081288e-03 -6.4551616e-03 -1.4280510e-03  6.4491653e-03\n",
      " -4.6173059e-03 -3.9930656e-03  4.9244044e-03  2.7130984e-03\n",
      " -1.8479753e-03 -2.8769434e-03  6.0107317e-03 -5.7167388e-03\n",
      " -3.2367026e-03 -6.4878250e-03 -4.2346325e-03 -8.5809948e-03\n",
      " -4.4697891e-03 -8.5112294e-03  1.4037776e-03 -8.6181965e-03\n",
      " -9.9166557e-03 -8.2016252e-03 -6.7726658e-03  6.6805850e-03\n",
      "  3.7845564e-03  3.5616636e-04 -2.9579818e-03 -7.4283206e-03\n",
      "  5.3341867e-04  4.9989222e-04  1.9561886e-04  8.5259555e-04\n",
      "  7.8633073e-04 -6.8160298e-05 -8.0070542e-03 -5.8702733e-03\n",
      " -8.3829118e-03 -1.3120425e-03  1.8206370e-03  7.4171280e-03\n",
      " -1.9634271e-03 -2.3252917e-03  9.4871549e-03  7.9704521e-05\n",
      " -2.4045217e-03  8.6048469e-03  2.6870037e-03 -5.3439722e-03\n",
      "  6.5881060e-03  4.5101536e-03 -7.0544672e-03 -3.2317400e-04\n",
      "  8.3448651e-04  5.7473574e-03 -1.7176545e-03 -2.8065301e-03\n",
      "  1.7484308e-03  8.4717153e-04  1.1928272e-03 -2.6342822e-03\n",
      " -5.9857843e-03  7.3229838e-03  7.5873756e-03  8.2963575e-03\n",
      " -8.5988473e-03  2.6364254e-03 -3.5599626e-03  9.6204039e-03\n",
      "  2.9037679e-03  4.6411133e-03  2.3856151e-03  6.6084778e-03\n",
      " -5.7432903e-03  7.8944126e-03 -2.4109220e-03 -4.5618857e-03\n",
      " -2.0609903e-03  9.7335577e-03 -6.8565905e-03 -2.1917201e-03\n",
      "  7.0009995e-03 -5.5749417e-05 -6.2949671e-03 -6.3935257e-03\n",
      "  8.9403950e-03  6.4295758e-03  4.7735930e-03 -3.2620477e-03\n",
      " -9.2676198e-03  3.7868882e-03  7.1605504e-03 -5.6328895e-03\n",
      " -7.8650126e-03 -2.9727400e-03 -4.9318983e-03 -2.3151112e-03]\n",
      "this: [-1.9434640e-03 -5.2691204e-03  9.4496058e-03 -9.3008578e-03\n",
      "  4.5043458e-03  5.4050456e-03 -1.4092061e-03  9.0091061e-03\n",
      "  9.8868310e-03 -5.4768636e-03 -6.0213986e-03 -6.7484509e-03\n",
      " -7.8960154e-03 -3.0481727e-03 -5.5953451e-03 -8.3461516e-03\n",
      "  7.8397384e-04  2.9956226e-03  6.4161662e-03 -2.6307860e-03\n",
      " -4.4544507e-03  1.2496916e-03  3.9263113e-04  8.1183659e-03\n",
      "  1.8241795e-04  7.2336327e-03 -8.2664695e-03  8.4348796e-03\n",
      " -1.8890941e-03  8.7032039e-03 -7.6182820e-03  1.7975287e-03\n",
      "  1.0570943e-03  4.6461497e-05 -5.1045078e-03 -9.2492709e-03\n",
      " -7.2662327e-03 -7.9535712e-03  1.9131829e-03  4.7796028e-04\n",
      " -1.8128387e-03  7.1211252e-03 -2.4759402e-03 -1.3476524e-03\n",
      " -8.9022741e-03 -9.9272644e-03  8.9503666e-03 -5.7559991e-03\n",
      " -6.3747931e-03  5.2003399e-03  6.6708666e-03 -6.8337950e-03\n",
      "  9.5918431e-04 -6.0095917e-03  1.6471200e-03 -4.2905998e-03\n",
      " -3.4421731e-03  2.1854921e-03  8.6632241e-03  6.7292359e-03\n",
      " -9.6793231e-03 -5.6238398e-03  7.8824637e-03  1.9902042e-03\n",
      " -4.2576035e-03  5.9976627e-04  9.5218150e-03 -1.1034268e-03\n",
      " -9.4269626e-03  1.6081010e-03  6.2337616e-03  6.2845801e-03\n",
      "  4.0922244e-03 -5.6506260e-03 -3.7027901e-04 -5.5318025e-05\n",
      "  4.5735650e-03 -8.0433870e-03 -8.0202818e-03  2.6542690e-04\n",
      " -8.6095063e-03  5.8213174e-03 -4.1825301e-04  9.9738948e-03\n",
      " -5.3456584e-03 -4.8652000e-04  7.7589322e-03 -4.0687295e-03\n",
      " -5.0168992e-03  1.5905675e-03  2.6514751e-03 -2.5654770e-03\n",
      "  6.4483341e-03 -7.6611158e-03  3.3938743e-03  4.9011729e-04\n",
      "  8.7350197e-03  5.9831389e-03  6.8169381e-03  7.8240465e-03]\n",
      "out: [-0.00950012  0.00956222 -0.00777076 -0.00264551 -0.00490641 -0.0049667\n",
      " -0.00802359 -0.00778358 -0.00455321 -0.00127536 -0.00510299  0.00614054\n",
      " -0.00951662 -0.0053071   0.00943715  0.00699133  0.00767582  0.00423474\n",
      "  0.00050709 -0.00598114  0.00601878  0.00263503  0.00769943  0.00639384\n",
      "  0.00794257  0.00865741 -0.00989575 -0.0067557   0.00133757  0.0064403\n",
      "  0.00737382  0.00551698  0.00766163 -0.00512557  0.00658441 -0.00410837\n",
      " -0.00905534  0.00914168  0.0013314  -0.00275968 -0.00247784 -0.00422048\n",
      "  0.00481234  0.00440022 -0.00265336 -0.00734188 -0.00356585 -0.00033661\n",
      "  0.00609589 -0.00283734 -0.00012089  0.00087973 -0.00709565  0.002065\n",
      " -0.00143242  0.00280215  0.00484222 -0.00135202 -0.00278014  0.00773865\n",
      "  0.0050456   0.00671352  0.00451564  0.00866716  0.00747497 -0.00108189\n",
      "  0.00874764  0.00460172  0.00544063 -0.00138608 -0.00204132 -0.00442435\n",
      " -0.0085152   0.00303773  0.00888319  0.00891974 -0.00194235  0.00608616\n",
      "  0.00377972 -0.00429597  0.00204292 -0.00543789  0.00820889  0.00543291\n",
      "  0.00318443  0.00410257  0.00865715  0.00727203 -0.00083347 -0.00707277\n",
      "  0.00838047  0.00723358  0.00173047 -0.00134749 -0.00589009 -0.00453309\n",
      "  0.00864797 -0.00313511 -0.00633882  0.00987008]\n",
      "check: [ 7.7013010e-03  9.1194436e-03  1.1349504e-03 -8.3253728e-03\n",
      "  8.4257321e-03 -3.6960833e-03  5.7456801e-03  4.3919426e-03\n",
      "  9.6892621e-03 -9.2950417e-03  9.2122182e-03 -9.2816344e-03\n",
      " -6.9086943e-03 -9.0985103e-03 -5.5499845e-03  7.3692640e-03\n",
      "  9.1697788e-03 -3.3224283e-03  3.7241264e-03 -3.6297780e-03\n",
      "  7.8828335e-03  5.8662812e-03  4.7380813e-06 -3.6284532e-03\n",
      " -7.2242348e-03  4.7701388e-03  1.4523285e-03 -2.6159808e-03\n",
      "  7.8387428e-03 -4.0511186e-03 -9.1465637e-03 -2.2513927e-03\n",
      "  1.2482972e-04 -6.6384124e-03 -5.4900437e-03 -8.4996894e-03\n",
      "  9.2275133e-03  7.4206130e-03 -2.9809965e-04  7.3640151e-03\n",
      "  7.9550464e-03 -7.8649592e-04  6.6167754e-03  3.7643660e-03\n",
      "  5.0790166e-03  7.2585153e-03 -4.7436589e-03 -2.1904972e-03\n",
      "  8.7110489e-04  4.2352648e-03  3.3045195e-03  5.0921110e-03\n",
      "  4.5826812e-03 -8.4378682e-03 -3.1881072e-03 -7.2418847e-03\n",
      "  9.6813189e-03  5.0028553e-03  1.6735455e-04  4.1092001e-03\n",
      " -7.6569906e-03 -6.2974072e-03  3.0798628e-03  6.5359371e-03\n",
      "  3.9486620e-03  6.0215225e-03 -1.9892624e-03 -3.3477666e-03\n",
      "  2.0378825e-04 -3.1967482e-03 -5.5183936e-03 -7.7862237e-03\n",
      "  6.5347673e-03 -1.0879923e-03 -1.8876848e-03 -7.8091677e-03\n",
      "  9.3405982e-03  8.7130180e-04  1.7684299e-03  2.4951715e-03\n",
      " -7.3836190e-03  1.6358088e-03  2.9736343e-03 -8.5667390e-03\n",
      "  4.9555334e-03  2.4307237e-03  7.5031454e-03  5.0422722e-03\n",
      " -3.0352010e-03 -7.1636932e-03  7.0955558e-03  1.9019847e-03\n",
      "  5.1980233e-03  6.3815522e-03  1.9121042e-03 -6.1278078e-03\n",
      " -2.3894390e-06  8.2662962e-03 -6.1000334e-03  9.4418591e-03]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing URLs, special characters, and unnecessary whitespaces.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the cleaned text into words using Python's built-in methods.\n",
    "    \"\"\"\n",
    "    # Split text by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def train_word2vec(sentences, vector_size=100, window=5, min_count=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the provided sentences.\n",
    "\n",
    "    Parameters:\n",
    "        sentences (list of list of str): Tokenized sentences.\n",
    "        vector_size (int): Dimensionality of the word vectors.\n",
    "        window (int): Maximum distance between the current and predicted word.\n",
    "        min_count (int): Ignores all words with total frequency lower than this.\n",
    "\n",
    "    Returns:\n",
    "        Word2Vec: Trained Word2Vec model.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, sg=1)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    Check out this amazing blog at https://example.com!\n",
    "    Contact us via email: test@example.com or call +123-456-7890.\n",
    "    #Python #NLP :)\n",
    "    \"\"\"\n",
    "    print(\"Original Text:\\n\", sample_text)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(sample_text)\n",
    "    print(\"\\nCleaned Text:\\n\", cleaned_text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenize_text(cleaned_text)\n",
    "    print(\"\\nTokenized Text:\\n\", tokenized_text)\n",
    "\n",
    "    # Prepare data for Word2Vec (list of tokenized sentences)\n",
    "    sentences = [tokenized_text]  # List of sentences, each sentence is a list of tokens\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    word2vec_model = train_word2vec(sentences)\n",
    "    print(\"\\nWord2Vec Embeddings:\")\n",
    "    for word in word2vec_model.wv.index_to_key:\n",
    "        print(f\"{word}: {word2vec_model.wv[word]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 編碼 (Encoding) -- Word emgedding(GloVe)\n",
    "Download pre-trained word vectors from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "    Check out this amazing blog at https://example.com!\n",
      "    Contact us via email: test@example.com or call +123-456-7890.\n",
      "    #Python #NLP :)\n",
      "    \n",
      "\n",
      "Cleaned Text:\n",
      " check out this amazing blog at contact us via email testexamplecom or call python nlp\n",
      "\n",
      "Tokenized Text:\n",
      " ['check', 'out', 'this', 'amazing', 'blog', 'at', 'contact', 'us', 'via', 'email', 'testexamplecom', 'or', 'call', 'python', 'nlp']\n",
      "\n",
      "Loading GloVe embeddings...\n",
      "\n",
      "Sentence Embedding:\n",
      " [ 0.28071099  0.21005214  0.21466415  0.15691272  0.18694929 -0.11066272\n",
      " -0.463882   -0.355695    0.0805194   0.04124065 -0.17067784  0.21215452\n",
      "  0.05459714 -0.10500858  0.32907079  0.02541357 -0.26532748 -0.16344179\n",
      " -0.30950393 -0.11786607  0.22847564  0.215925    0.42892586  0.24820764\n",
      "  0.323294   -1.00813928  0.03517129 -0.14934993  0.23746585 -0.37186436\n",
      "  2.52624642  0.07874621 -0.3274235  -0.08321058 -0.16669921 -0.04749864\n",
      "  0.06699258 -0.15182529  0.05304036 -0.10111173  0.55109912  0.08533421\n",
      " -0.09511235  0.36200704  0.13417021 -0.02908129  0.27668578  0.15528814\n",
      "  0.09340957  0.22576607]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing URLs, special characters, and unnecessary whitespaces.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the cleaned text into words using Python's built-in methods.\n",
    "    \"\"\"\n",
    "    # Split text by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def load_glove_embeddings(glove_file_path):\n",
    "    \"\"\"\n",
    "    Loads GloVe embeddings from a file.\n",
    "\n",
    "    Parameters:\n",
    "        glove_file_path (str): Path to the GloVe embeddings file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping words to their GloVe embeddings.\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefficients = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefficients\n",
    "    return embeddings_index\n",
    "\n",
    "def get_sentence_embedding(tokens, embeddings_index):\n",
    "    \"\"\"\n",
    "    Computes the sentence embedding by averaging GloVe embeddings of words.\n",
    "\n",
    "    Parameters:\n",
    "        tokens (list): List of tokens from the sentence.\n",
    "        embeddings_index (dict): GloVe word embeddings dictionary.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The sentence embedding as a vector.\n",
    "    \"\"\"\n",
    "    embedding_dim = len(next(iter(embeddings_index.values())))  # Dimension of GloVe embeddings\n",
    "    sentence_embedding = np.zeros(embedding_dim)\n",
    "    valid_tokens = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in embeddings_index:\n",
    "            sentence_embedding += embeddings_index[token]\n",
    "            valid_tokens += 1\n",
    "\n",
    "    if valid_tokens > 0:\n",
    "        sentence_embedding /= valid_tokens\n",
    "\n",
    "    return sentence_embedding\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    Check out this amazing blog at https://example.com!\n",
    "    Contact us via email: test@example.com or call +123-456-7890.\n",
    "    #Python #NLP :)\n",
    "    \"\"\"\n",
    "    print(\"Original Text:\\n\", sample_text)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(sample_text)\n",
    "    print(\"\\nCleaned Text:\\n\", cleaned_text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenize_text(cleaned_text)\n",
    "    print(\"\\nTokenized Text:\\n\", tokenized_text)\n",
    "\n",
    "    # Load GloVe embeddings (download GloVe file and provide the correct path)\n",
    "    glove_file_path = \"../data/glove.6B/glove.6B.50d.txt\"  # Example file: 50-dimensional GloVe vectors\n",
    "    print(\"\\nLoading GloVe embeddings...\")\n",
    "    glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "    # Compute sentence embedding\n",
    "    sentence_embedding = get_sentence_embedding(tokenized_text, glove_embeddings)\n",
    "    print(\"\\nSentence Embedding:\\n\", sentence_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預訓練嵌入（ BERT, Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "### 什麼是 BERT？\n",
    "\n",
    "BERT（**Bidirectional Encoder Representations from Transformers**）是一個由 Google 開發的強大自然語言處理（NLP）模型，於 2018 年由 Jacob Devlin 等人在論文 [\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"](https://arxiv.org/abs/1810.04805) 中首次提出。BERT 在許多 NLP 任務中（如問題回答、情感分析和命名實體識別）達到了最先進的效果，並徹底改變了 NLP 的發展。\n",
    "\n",
    "---\n",
    "\n",
    "### BERT 的主要特點\n",
    "\n",
    "1. **雙向語境理解**：\n",
    "   - BERT 是**雙向的**，可以同時從左到右和從右到左閱讀文本，全面理解單詞的上下文。\n",
    "   - 傳統模型（如 RNN 或早期的 Transformer）通常是單向的，無法充分理解完整語境。\n",
    "\n",
    "2. **基於 Transformer 的架構**：\n",
    "   - BERT 基於 Transformer 模型，它使用**自注意力機制（Self-Attention Mechanism）**來高效處理輸入文本並捕捉單詞之間的關係，無論它們之間的距離有多遠。\n",
    "\n",
    "3. **預訓練與微調（Fine-tuning）**：\n",
    "   - **預訓練**：\n",
    "     - BERT 在大型文本語料庫（如 Wikipedia 和 BooksCorpus）上進行預訓練，採用以下兩種無監督任務：\n",
    "       1. **遮罩語言模型（Masked Language Modeling, MLM）**：隨機遮罩文本中的部分單詞，並讓模型預測這些單詞。\n",
    "       2. **下一句預測（Next Sentence Prediction, NSP）**：預測兩個句子是否邏輯上相連。\n",
    "   - **微調**：\n",
    "     - 預訓練完成後，BERT 可以根據具體任務（如情感分析、機器翻譯）使用有標籤的數據進行微調。\n",
    "\n",
    "4. **預訓練模型的不同版本**：\n",
    "   - `bert-base`：12 層（Transformer blocks），768 個隱藏單元，12 個注意力頭，共 1.1 億參數。\n",
    "   - `bert-large`：24 層，1024 個隱藏單元，16 個注意力頭，共 3.4 億參數。\n",
    "\n",
    "---\n",
    "\n",
    "### 為什麼 BERT 很重要？\n",
    "\n",
    "1. **最先進的性能**：\n",
    "   - BERT 在多個 NLP 基準任務（如 GLUE 基準測試、SQuAD 問答數據集等）上創造了新紀錄。\n",
    "\n",
    "2. **語境化詞嵌入**：\n",
    "   - 傳統詞嵌入模型（如 Word2Vec 或 GloVe）為單詞生成固定的嵌入，而 BERT 的嵌入會根據上下文動態變化。\n",
    "   - 例如，“bank” 在 “river bank” 和 “financial bank” 中的嵌入不同。\n",
    "\n",
    "3. **廣泛的應用**：\n",
    "   - BERT 被應用於多種任務：\n",
    "     - 文本分類（如情感分析）。\n",
    "     - 命名實體識別（NER）。\n",
    "     - 問答系統（如 SQuAD）。\n",
    "     - 文本摘要和改寫。\n",
    "\n",
    "---\n",
    "\n",
    "### BERT 的工作原理\n",
    "\n",
    "#### 1. **輸入表示**\n",
    "   - BERT 的輸入包含以下部分：\n",
    "     - **詞嵌入（Token Embeddings）**：表示單詞或子詞（WordPiece Tokenization）。\n",
    "     - **段落嵌入（Segment Embeddings）**：標識輸入的句子邊界（用於句子對任務）。\n",
    "     - **位置嵌入（Positional Embeddings）**：編碼每個單詞在序列中的位置。\n",
    "\n",
    "#### 2. **自注意力機制（Self-Attention）**\n",
    "   - BERT 的 Transformer 架構通過自注意力機制來捕捉句子中所有單詞的關聯，無論距離遠近。\n",
    "\n",
    "#### 3. **預訓練任務**\n",
    "   - **遮罩語言模型（MLM）**：\n",
    "     - 隨機遮罩 15% 的單詞，讓模型預測被遮罩的單詞。\n",
    "   - **下一句預測（NSP）**：\n",
    "     - 判斷兩個句子是否邏輯上相連。\n",
    "\n",
    "#### 4. **微調**\n",
    "   - 在預訓練完成後，BERT 可以通過微調適配特定的 NLP 任務。\n",
    "\n",
    "---\n",
    "\n",
    "### BERT 的應用\n",
    "\n",
    "1. **問題回答（Question Answering）**：\n",
    "   - 給定問題和上下文，BERT 可以提取準確的答案範圍。\n",
    "2. **文本分類（Text Classification）**：\n",
    "   - 任務如情感分析、垃圾郵件檢測和主題分類。\n",
    "3. **命名實體識別（NER）**：\n",
    "   - 提取文本中的命名實體（如人名、日期、地點）。\n",
    "4. **搜索引擎**：\n",
    "   - Google 使用 BERT 提升對搜索查詢的理解。\n",
    "\n",
    "---\n",
    "\n",
    "### BERT 的優勢\n",
    "\n",
    "- 雙向上下文感知，理解單詞的完整語境。\n",
    "- 在多個 NLP 任務上實現最先進的性能。\n",
    "- 開放源代碼，且易於使用（如 Hugging Face 的 Transformers 庫）。\n",
    "- 可以通過微調輕鬆遷移到具體任務。\n",
    "\n",
    "### BERT 的挑戰\n",
    "\n",
    "- **計算成本高**：訓練和推理都需要大量計算資源。\n",
    "- **內存需求大**：特別是 `bert-large` 等較大版本。\n",
    "- **微調需要足夠的數據**：對於小型數據集可能效果有限。\n",
    "\n",
    "---\n",
    "\n",
    "### 小結\n",
    "\n",
    "BERT 結合了雙向上下文、Transformer 架構和創新的預訓練策略，使其在自然語言理解中取得了重大突破。它已成為許多 NLP 模型和應用的基礎，為搜索引擎、對話系統和文本分類等應用提供強大的支持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "    Check out this amazing blog at https://example.com!\n",
      "    Contact us via email: test@example.com or call +123-456-7890.\n",
      "    #Python #NLP :)\n",
      "    \n",
      "\n",
      "Cleaned Text:\n",
      " check out this amazing blog at contact us via email testexamplecom or call python nlp\n",
      "\n",
      "Tokenized Text:\n",
      " ['check', 'out', 'this', 'amazing', 'blog', 'at', 'contact', 'us', 'via', 'email', 'testexamplecom', 'or', 'call', 'python', 'nlp']\n",
      "\n",
      "Loading BERT model...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nBertModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFBertModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoading BERT model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Get BERT embeddings\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerating BERT embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/113_Python/lib/python3.11/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/113_Python/lib/python3.11/site-packages/transformers/utils/import_utils.py:1645\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[0;31mImportError\u001b[0m: \nBertModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFBertModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing URLs, special characters, and unnecessary whitespaces.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the cleaned text into words using Python's built-in methods.\n",
    "    \"\"\"\n",
    "    # Split text by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def get_bert_embeddings(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates BERT embeddings for the given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Input text.\n",
    "        model: Pretrained BERT model.\n",
    "        tokenizer: BERT tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The BERT embeddings for the input text.\n",
    "    \"\"\"\n",
    "    # Tokenize and encode input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the embeddings (last hidden state)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    return embeddings\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    Check out this amazing blog at https://example.com!\n",
    "    Contact us via email: test@example.com or call +123-456-7890.\n",
    "    #Python #NLP :)\n",
    "    \"\"\"\n",
    "    print(\"Original Text:\\n\", sample_text)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(sample_text)\n",
    "    print(\"\\nCleaned Text:\\n\", cleaned_text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenize_text(cleaned_text)\n",
    "    print(\"\\nTokenized Text:\\n\", tokenized_text)\n",
    "\n",
    "    # Load BERT model and tokenizer\n",
    "    print(\"\\nLoading BERT model...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Get BERT embeddings\n",
    "    print(\"\\nGenerating BERT embeddings...\")\n",
    "    embeddings = get_bert_embeddings(cleaned_text, model, tokenizer)\n",
    "    print(\"\\nBERT Embeddings Shape:\", embeddings.shape)\n",
    "    print(\"BERT Embeddings (First Token):\", embeddings[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7921696305274963\n",
      "Epoch 10, Loss: 0.04010596126317978\n",
      "Epoch 20, Loss: 0.008932550437748432\n",
      "Epoch 30, Loss: 0.0038160928525030613\n",
      "Epoch 40, Loss: 0.0023599162232130766\n",
      "Epoch 50, Loss: 0.0017662409227341413\n",
      "Epoch 60, Loss: 0.0014507893938571215\n",
      "Epoch 70, Loss: 0.0012470402289181948\n",
      "Epoch 80, Loss: 0.0010971329174935818\n",
      "Epoch 90, Loss: 0.0009782267734408379\n",
      "Test Accuracy: 100.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the RNN Model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, hidden = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Use the last hidden state for classification\n",
    "        return out\n",
    "\n",
    "# Parameters\n",
    "input_size = 1   # Each number in the sequence\n",
    "hidden_size = 16 # Number of hidden units\n",
    "output_size = 2  # Two classes: even (0), odd (1)\n",
    "\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training Data with Padding\n",
    "inputs = torch.tensor([\n",
    "    [[1], [2], [3]],  # Sequence 1\n",
    "    [[4], [6], [0]]   # Sequence 2 (padded with 0)\n",
    "], dtype=torch.float32)\n",
    "labels = torch.tensor([0, 0])  # 0 for even, 1 for odd\n",
    "\n",
    "# Test Data\n",
    "test_inputs = torch.tensor([\n",
    "    [[3], [5], [0]],  # Sum = 8 (even)\n",
    "    [[2], [4], [6]]   # Sum = 12 (even)\n",
    "], dtype=torch.float32)\n",
    "test_labels = torch.tensor([0, 0])  # 0 for even, 1 for odd\n",
    "\n",
    "# Training Loop\n",
    "epochs = 100\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_inputs)\n",
    "    predictions = torch.argmax(test_outputs, dim=1)\n",
    "    accuracy = (predictions == test_labels).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(range(epochs), loss_history, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Step 2：**序列處理**\n",
    "1. **填充與截斷（Padding and Truncation）**：\n",
    "   - 確保所有序列的長度一致，短序列用填充符（如 0）補齊，長序列則截斷。\n",
    "2. **批量處理（Batch Preparation）**：\n",
    "   - 將資料集劃分為訓練集、驗證集和測試集，並生成小批量資料以提高計算效率。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3：**RNN 模型架構**\n",
    "1. **輸入層（Input Layer）**：\n",
    "   - 接收數字化表示的序列資料。\n",
    "2. **嵌入層（Embedding Layer）**：\n",
    "   - 如果未使用預訓練嵌入，則將輸入數字轉換為密集向量表示。\n",
    "3. **RNN 層**：\n",
    "   - 負責處理序列資料。常見變體包括：\n",
    "     - 簡單 RNN\n",
    "     - 長短期記憶（LSTM）網絡\n",
    "     - 閘門循環單元（GRU）。\n",
    "4. **全連接層（Dense Layer）**：\n",
    "   - 將 RNN 的輸出映射到目標維度（例如分類任務中的類別數）。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4：**損失函數**\n",
    "- 根據任務選擇適合的損失函數：\n",
    "  - **交叉熵損失（Cross-Entropy Loss）**：用於分類任務。\n",
    "  - **均方誤差（Mean Squared Error）**：用於回歸任務。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5：**優化器選擇**\n",
    "- 使用如 Adam、SGD 或 RMSProp 等優化器，幫助最小化損失。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6：**模型訓練**\n",
    "1. **前向傳播（Forward Propagation）**：\n",
    "   - 將輸入資料通過 RNN，生成預測結果。\n",
    "2. **時間反向傳播（BPTT，Backpropagation Through Time）**：\n",
    "   - 計算序列的梯度並更新權重。\n",
    "3. **多次訓練（Epochs）**：\n",
    "   - 對多個小批量進行多輪訓練以保證模型收斂。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7：**模型評估**\n",
    "- 使用驗證資料進行評估。\n",
    "- 根據任務需求，評估指標如準確率（Accuracy）、F1分數、或 BLEU 分數（機器翻譯）等。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 8：**模型微調**\n",
    "- 調整超參數，如學習率、Dropout 比例、RNN 層數等。\n",
    "- 如果使用了預訓練嵌入，可對特定任務進行微調。\n",
    "\n",
    "---\n",
    "\n",
    "### Step 9：**部署**\n",
    "- 保存訓練好的模型。\n",
    "- 在實際應用中進行推理，如文本分類、情感分析或翻譯。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('', 0), ('[UNK]', 1), ('i', 2), ('write', 3), ('erase', 4), ('rewrite', 5), ('again', 6), ('and', 7), ('then', 8), ('a', 9), ('poppy', 10), ('blooms', 11)])\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1 \n",
    "    return vector\n",
    "\n",
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text \n",
    "                        if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [           \n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)\n",
    "print( vectorizer.vocabulary.items() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7, 1, 5, 6]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1 \n",
    "    return vector\n",
    "\n",
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text \n",
    "                        if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [           \n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)\n",
    "\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write rewrite and [UNK] rewrite again'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1 \n",
    "    return vector\n",
    "\n",
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text \n",
    "                        if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [           \n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)\n",
    "\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "\n",
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AclImdb dataset\n",
    "\n",
    "prepare a validation set by setting apart 20% of the training text files in a new directory, `aclImdb/val`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset\n",
    "The Stanford IMDb dataset is available in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Load the dataset\n",
    "max_features = 10000  # Vocabulary size\n",
    "max_len = 500         # Maximum length of sequences\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences to make them of equal length\n",
    "train_data = pad_sequences(train_data, maxlen=max_len)\n",
    "test_data = pad_sequences(test_data, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the RNN Model\n",
    "An RNN model is suitable for sequence data like text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/113_Python/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-12-03 14:41:11.850950: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-12-03 14:41:11.850982: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-12-03 14:41:11.850990: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-12-03 14:41:11.851010: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-03 14:41:11.851024: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=128, input_length=max_len),\n",
    "    SimpleRNN(128, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the RNN Model\n",
    "Use the training data to train the RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 14:41:24.006670: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-12-03 14:41:24.020396: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] PluggableGraphOptimizer failed: INVALID_ARGUMENT: Failed to deserialize the `graph_buf`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  6/313\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11:05:40\u001b[0m 130s/step - accuracy: 0.4589 - loss: 0.7181"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_data, \n",
    "    train_labels, \n",
    "    epochs=5, \n",
    "    batch_size=64, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Model\n",
    "Test the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize the Training Process\n",
    "You can visualize the accuracy and loss over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# following parts just for test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"../data/aclImdb\")\n",
    "train_dir = base_dir / \"train\"\n",
    "val_dir = base_dir / \"val\"\n",
    "for category in ( \"neg\", \"pos\" ):\n",
    "    if not( ( pathlib.Path.cwd() / val_dir / category ).exists() ):\n",
    "        os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, \n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing groups of words\n",
    "\n",
    "`following code run one time only.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70000 files belonging to 3 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"../data/aclImdb\")\n",
    "train_dir = base_dir / \"train\"\n",
    "val_dir = base_dir / \"val\"\n",
    "test_dir = base_dir / \"test\"\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    train_dir, batch_size=batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    val_dir, batch_size=batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    test_dir, batch_size=batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.target: <dtype: 'int32'>\n",
      "inputs[0]: b'Dear Friends and Family,<br /><br />I guess if one teen wants to become biblical with another teen, then that\\'s their eternal damnation - just remember kids, \"birth control\" doesn\\'t mean \"oral sex\", I don\\'t care what the honor student says. On the other hand, even if the senator\\'s aid quotes himself as a \"bit of a romantic guy\", he\\'s still only hitting on a high school girl. If she was my sister, I\\'d eat this guys kneecaps.<br /><br />Other than that I found out that Mongolians don\\'t kiss the same way the French do and that baseball players named Zoo like delicate undergarments.<br /><br />I think I\\'d almost rather watch Richie Rich one more time than suffer the indignity of this slip, slap, slop. Thank you, and good night.'\n",
      "targets[0]: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 14:08:55.786257: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print( f\"inputs.shape: {inputs.shape}\" )\n",
    "    print( f\"inputs.dtype: {inputs.dtype}\" )\n",
    "    print( f\"targets.shape: {targets.shape}\" )\n",
    "    print( f\"targets.target: {targets.dtype}\" )\n",
    "    print( f\"inputs[0]: {inputs[0]}\" )\n",
    "    print( f\"targets[0]: {targets[0]}\" )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 14:15:06.144300: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Cannot batch tensors with different shapes in component 0. First element had shape [32] and element 11 had shape [16].\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 0. First element had shape [32] and element 11 had shape [16]. [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m text_only_train_ds \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: tf\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Adapt the vectorization layer\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtext_vectorization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_only_train_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Apply TextVectorization to datasets\u001b[39;00m\n\u001b[1;32m     21\u001b[0m binary_1gram_train_ds \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x, y: (text_vectorization(tf\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), y),\n\u001b[1;32m     23\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/113_Python/lib/python3.11/site-packages/keras/src/layers/preprocessing/text_vectorization.py:420\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtake(steps)\n\u001b[0;32m--> 420\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/113_Python/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:809\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    808\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/113_Python/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:772\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 772\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/113_Python/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3086\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3085\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3086\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3088\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/113_Python/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 0. First element had shape [32] and element 11 had shape [16]. [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the TextVectorization layer\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000, output_sequence_length=500\n",
    ")\n",
    "\n",
    "# Prepare datasets (ensure proper batching)\n",
    "train_ds = train_ds.batch(32).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(32).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(32).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Fix shape by expanding dimensions\n",
    "text_only_train_ds = train_ds.map(lambda x, y: tf.expand_dims(x, axis=-1))\n",
    "\n",
    "# Adapt the vectorization layer\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# Apply TextVectorization to datasets\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(tf.expand_dims(x, axis=-1)), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(tf.expand_dims(x, axis=-1)), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(tf.expand_dims(x, axis=-1)), y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.target: <dtype: 'int32'>\n",
      "inputs[0]: b'As far as serial- killer films and thrillers are concerned this one\\'s right down there with the worst of \\'em. Only \"Copycat\" and \"Sisters\" manage to be more annoying in their absurdity, and only \"Saw\" is worse. And it\\'s a pity because the first hour is genuinely eerie, with a fittingly claustrophobic mortuary as the ideal setting. I am a very jaded viewer, but some of those early mortuary scenes really get under your skin. However, after an hour it all goes downhill, and I mean steeply. The relentlessly stupid plot twists rob you of all patience and the movie just keeps sinking to new lows. All logic is thrown into the wind, and the viewer\\'s intelligence is insulted and pounded upon repeatedly with more force than that baseball bat could ever have generated - the one used by Nolte and McGregor. (I\\'d be the first to sign up if they were looking for volunteers to take that baseball bat and bash the heads of the writers of this nonsense.) Nothing here adds up. Absolutely nothing. Nolte was molesting corpses decades earlier and the writers of this film would have us believe that this man could years later become the city\\'s chief police investigator! Making him the killer is as absurd as giving Thomas Edison credit for inventing the wheel, as laughable as a conspiracy-theory plot from the \"X-Files\", and as stupid as Kim Basinger\\'s book on how to solve all of world\\'s problems (if she\\'d ever write one). The very notion that a man - so disturbed that he indulges in necrophilia in mortuaries - would have the sanity, interest, patience, and willingness to climb all the way to chief investigator in a police department only to start a savage murder spree is simply a mind-bogglingly dumb, far-fetched concept to me.<br /><br />And how the hell did he even start with the framing of McGregor? This is an essential piece of the puzzle that is badly missing here; McGregor JUST HAPPENS to get a job where he meets Nolte. And McGregor\\'s best friend, Brolin, JUST HAPPENS to know a prostitute who is Nolte\\'s play toy (and later victim). It can sure be a small, small world in a Hollywood stinker! And to add some silliness, Brolin is some kind of a deranged thrill-seeker who acts like a total lunatic all the time. Obviously, he is the decoy for the viewer; we are meant to treat him as the suspect. But how dumb do they think we are? And how the hell did Brolin get into the mortuary when he carried out his \"practical joke\"? And how the hell did Nolte manage to drag out a body of one of his victims within seconds of McGregor entering the room, without McGregor noticing it (the fact that he had his walkman on and/or was singing and/or talking doesn\\'t make it any more believable)? And what\\'s with this annoying scene where Arquette JUST HAPPENS to walk into the mortuary at exactly the moment when McGregor is hitting Nolte with a baseball bat and predictably starts thinking her boyfriend is the killer??! More annoyances came from the scene in which Brolin\\'s reaction to McGregor\\'s telling him that the latter is been suspected of murder is to laugh! Or the one in which he cuts off his own thumb in order to free himself and save the others. Sure,... why not?? (\"I am being hand-cuffed to this metal pole, and as a result can neither save my friends nor myself... What do I do...? I know!... I\\'ll cut my thumb off! How come I didn\\'t think of that before!?...\") Nolte makes the best out of his role, but due to the bitchingly silly script he appears to be hamming it up too much at the end - but what choice did he have? McGregor is solid, too, apart from his on-and-off accent (which was it now? American or English?).'\n",
      "targets[0]: 2\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print( f\"inputs.shape: {inputs.shape}\" )\n",
    "    print( f\"inputs.dtype: {inputs.dtype}\" )\n",
    "    print( f\"targets.shape: {targets.shape}\" )\n",
    "    print( f\"targets.target: {targets.dtype}\" )\n",
    "    print( f\"inputs[0]: {inputs[0]}\" )\n",
    "    print( f\"targets[0]: {targets[0]}\" )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100006 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 13:36:43.988070: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text \n",
    "                        if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "dataset = keras.utils.text_dataset_from_directory(\n",
    "    directory=\"../data/aclImdb\",\n",
    "    label_mode=None,\n",
    "    batch_size=256\n",
    ")\n",
    "dataset = dataset.map( lambda x: tf.strings.regex_replace( x, \"<br />\", \" \") )\n",
    "vectorizer = Vectorizer()\n",
    "vocabulary = {}\n",
    "for text_batch in dataset:\n",
    "    for text in text_batch.numpy():  # Ensure to iterate over batch\n",
    "        if isinstance(text, bytes):  # Check if it's a byte string\n",
    "            text = text.decode('utf-8')  # Decode only if necessary\n",
    "        standardized_text = vectorizer.standardize(text)\n",
    "        tokens = vectorizer.tokenize(standardized_text)\n",
    "        for token in tokens:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = len(vocabulary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "113_Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
